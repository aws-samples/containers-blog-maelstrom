{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the Amazon Containers Blog Maelstrom documentation site.</p> <p>This repository accompaniment to blog posts published on the AWS Containers Blog. It can be used by AWS customers, partners, and internal AWS teams to configure and manage complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. </p>"},{"location":"ALB-ingresssharing-tbg/","title":"ALB Ingress Sharing and Target Group Binding","text":""},{"location":"ALB-ingresssharing-tbg/#a-deeper-look-at-ingress-sharing-and-target-group-binding-in-aws-load-balancer-controller","title":"A deeper look at Ingress Sharing and Target Group Binding in AWS Load Balancer Controller","text":"<p>AWS Load Balancer Controlleris a Kubernetes controller that integrates Application Load Balancers (ALB) and Network Load Balancers (NLB) with Kubernetes workloads. It allows you to configure and manage load balancers using Kubernetes API. Based on our conversations with customers, we have identified two AWS Load Balancer Controller features that need further explanation. In this post, we show how you can reduce costs by using Ingress Grouping and integrate existing load balancers using target group binding. </p> <p>Kubernetes has two ways to expose Services to clients external to the cluster. Kubernetes Service of type <code>LoadBalancer</code> or <code>Ingress</code> resources. Both methods rely on AWS Elastic Load Balancing under the hood. Network Load Balancer (NLB) is designed to handle high amounts of traffic and is optimized to handle layer 4 (TCP and UDP) traffic. And Application Load Balancer (ALB) is designed to handle web traffic and is optimized to handle layer 7 (HTTP and HTTPS) traffic.   The AWS Load Balancer Controller automates the management of ALBs and NLBs' lifecycle and configuration as you deploy workloads in your cluster.</p>"},{"location":"ALB-ingresssharing-tbg/#kubernetes-ingress-and-application-load-balancer","title":"Kubernetes Ingress and Application Load Balancer","text":"<p>In Kubernetes, Ingress is an API object that provides an external, load-balanced IP address to access the services in a cluster. It acts as a layer 7 (HTTP/HTTPS) reverse proxy and allows you to route traffic to different services based on the requested host and URL path. The AWS Load Balancer controller provisions and configures an ALB on your behalf whenever you create an Ingress. </p> <p>In a microservice architecture, it is common to have multiple services deployed within a single Kubernetes cluster. Each service may have different requirements for external access, routing rules, SSL/TLS termination, and so on. In such cases, it may be necessary to use multiple Ingress resources to manage external access to the services. Applications typically include Ingress resource definition in deployment artifacts (along with Deployments, Services, Volumes, etc.) since they contain application-specific routing rules. Separating Ingress resources is a good practice as it allows teams to modify their Ingress resources without affecting traffic routing for other applications in the cluster. </p> <p>Although using the AWS Load Balancer Controller for Ingresses can be beneficial, there is a disadvantage to this approach. The controller creates an Application Load Balancer (ALB) for each Ingress, which can result in a higher number of load balancers than necessary. This can lead to increased costs since each ALB incurs an hourly charge. Having a separate load balancer for each Service may quickly become too expensive. However, you can reduce costs by sharing ALBs across multiple Ingresses, thereby minimizing the number of ALBs needed.</p> <p>To minimize the number of Application Load Balancers (ALBs) in your architecture, you can use the AWS Load Balancer Controller to group Ingresses. The controller offers an IngressGroup feature that enables you to share a single ALB with multiple Ingress resources. This allows you to consolidate your load balancers and reduce costs. Additionally, the Ingress group can include Ingresses from different namespaces, making it easier to manage access to your microservices across multiple namespaces. </p>"},{"location":"ALB-ingresssharing-tbg/#ingress-groups-in-action","title":"Ingress groups in action","text":"<p>Let\u2019s walk through the process of sharing ALBs with multiple Ingresses. For this demonstration, we\u2019ll deploy four web applications in two different namespaces. Then, we\u2019ll show you how multiple Ingresses can be configured and grouped to share a single ALB. We\u2019ll use <code>[group.name](http://group.name/)</code> annotations to enable grouping of multiple Ingress resources. </p> <p>The diagram above shows the operations the AWS Load Balancer Controller performs once installed. It watches the Kubernetes API server for updates to Ingress resources. When it detects changes, it updates resources such as the Application Load Balancer, listeners, target groups, and listener rules.</p> <ul> <li>A Target group gets created for every Kubernetes Service mentioned in the Ingress resource</li> <li>Listeners are created for every port defined in the Ingress resource\u2019s annotations</li> <li>Listener rules (also called ingress rules) are created for each path in Ingress resource definition</li> </ul> <p>In this post, we will run four variants of a web application that renders a web page with different background colors. The <code>blue</code> and <code>green</code> apps will run in the <code>blue-green-ns</code> namespace, and the <code>orange</code> and <code>purple</code> apps will run in the <code>orange-purple-ns</code> namespace. With apps deployed, we\u2019ll create two I<code>ngress</code> resources named <code>blue-green-ingress</code> and <code>orange-purple-ingress</code>. Ingress rules will configure path-based routing. </p> <p>In the diagram below, when ALB receives traffic, it will route requests to Pods based on the Ingress rules. The <code>blue-green-ingress</code> Ingress will have the routing rules to access <code>blue</code> and <code>green</code> web apps, and will be deployed in the <code>blue-green-ns</code> namespace. Similarly, the <code>orange-purple-ingress</code> Ingress will have the routing rules to access <code>orange</code> and <code>purple</code> web apps, and will be deployed in namespace <code>orange-purple-ns</code>.</p> <p></p>"},{"location":"ALB-ingresssharing-tbg/#solution-walkthrough","title":"Solution walkthrough","text":"<p>You\u2019ll need the following things to follow along:</p> <ul> <li>An existing EKS Cluster with an existing node group</li> <li>AWS Load Balancer Controller  is installed in the cluster</li> <li>Tools required on a machine with access to the AWS and Kubernetes API Server. This could be your local or a remote system or an AWS Cloud9 environment. You\u2019ll need these tools installed:<ul> <li>AWS CLI</li> <li>eksctl</li> <li>kubectl</li> <li>Helm</li> <li>Docker</li> </ul> </li> </ul> <p>The code is available on Github. Start by cloning the code and deploy the sample application: </p> <pre><code>`git clone https``:``//github.com/aws-samples/containers-blog-maelstrom.git``\nkubectl apply -f containers-blog-maelstrom/aws-lb-controller-blog/ingress-grouping/\n`\n</code></pre> <p>The output should show the resources you\u2019ve created:</p> <pre><code>namespace/blue-green-ns created\ndeployment.apps/green-app created\ndeployment.apps/blue-app created\nservice/green-service created\nservice/blue-service created\ningress.networking.k8s.io/blue-green-ingress created\nnamespace/orange-purple-ns created\ndeployment.apps/orange-app created\ndeployment.apps/purple-app created\nservice/orange-service created\nservice/purple-service created\ningress.networking.k8s.io/orange-purple-ingress created\n</code></pre> <p>Check the status of resources in namespace <code>blue-green-ns</code> . </p> <pre><code>kubectl -n blue-green-ns get all\n</code></pre> <p>The Pods should be in running state:</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/blue-app-9b7bd7578-gjqrm     1/1     Running   0          5d23h\npod/blue-app-9b7bd7578-sgjvd     1/1     Running   0          5d23h\npod/green-app-695664547f-lmq4b   1/1     Running   0          5d23h\npod/green-app-695664547f-lrjh8   1/1     Running   0          5d23h\n\nNAME                    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/blue-service    NodePort   172.20.93.229   &lt;none&gt;        80:32402/TCP   5d23h\nservice/green-service   NodePort   172.20.63.132   &lt;none&gt;        80:30106/TCP   5d23h\n\nNAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/blue-app    2/2     2            2           5d23h\ndeployment.apps/green-app   2/2     2            2           5d23h\n</code></pre> <p>Similarly, verify that the application Pods are operational in the  <code>orange-purple-ns</code> namespace.</p> <p>We also have two Ingress resources:</p> <pre><code>kubectl get ingress -A\n</code></pre> <p>You can see that both Ingress resources have the same <code>ADDRESS</code>:</p> <pre><code>NAMESPACE          NAME                    CLASS   HOSTS   ADDRESS                                                            PORTS   AGE\nblue-green-ns      blue-green-ingress      alb     *       k8s-appcolorlb-9527f4eb57-1779454964.us-west-2.elb.amazonaws.com   80      5m52s\norange-purple-ns   orange-purple-ingress   alb     *       k8s-appcolorlb-9527f4eb57-1779454964.us-west-2.elb.amazonaws.com   80      5m25s\n</code></pre> <p>We didn\u2019t end up with one ALB for each Ingress is because we have grouped Ingresses. Before looking further at the Ingress, let\u2019s verify the routing works the way we want it to. </p> <p>Get the address assigned to the ALB:</p> <pre><code>kubectl get ingress blue-green-ingress -n blue-green-ns \\\n  -o=jsonpath=\"{'http://'}{.status.loadBalancer.ingress[].hostname}{'\\n'}\"\n</code></pre> <p>Navigate to the address of the ALB at <code>/green</code> path. A webpage with Green background(as shown below) indicates that the routing is working as intended.  Similarly, the <code>/blue</code> , <code>/orange</code>, and <code>/purple</code> paths should show a page each with their corresponding background color.  Let\u2019s get back to the reason for having just one ALB for both Ingresses. Describe either of the Ingresses and you\u2019d notice that they include the <code>[alb.ingress.kubernetes.io/group.name](http://alb.ingress.kubernetes.io/group.name)</code> annotation. </p> <pre><code>kubectl -n blue-green-ns describe ingress blue-green-ingress\n</code></pre> <p>Below are the Ingress annotations from the blue-green-ingress.yaml:</p> <pre><code>  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/group.name: app-color-lb\n</code></pre> <p>By adding an <code>alb.ingress.kubernetes.io/group.name</code> annotation, you can assign a group to an Ingress. Ingresses with same <code>group.name</code> annotation form an \"IngressGroup\". An IngressGroup can have Ingresses in multiple namespaces. </p> <p>In our example, both <code>blue-green-ingress</code> and <code>orange-purple-ingress</code> use \"app-color-lb\" as the value of this annotation, which puts them in the same group. This enables the Application Load Balancer to route traffic for both the <code>Ingresse</code>s based on their corresponding routing rules. You can view the ingress rules as they are configured as listener rules in ALB. The screenshot below shows the <code>Rules</code> that the AWS Load Balancer Controller created:   With IngressGroup, a single ALB serves as the entry point for all four applications. This change in design reduces the number of ALBs we need and results in cost savings. </p>"},{"location":"ALB-ingresssharing-tbg/#design-considerations-with-ingressgroup","title":"Design considerations with IngressGroup","text":"<p>An important aspect to consider before using IngressGroup in a multi-tenant environment is conflict resolution. When AWS Load Balancer Controller configures ingress rules in ALB, it uses the <code>group.order</code> field to set the order of evaluation. If you don\u2019t declare a value for <code>group.order</code>, the Controller defaults to 0. </p> <p>ALB determines how to route requests by applying the ingress rules. The order of rule evaluation is set by the <code>group.order</code> field. Rules with lower order value are evaluated first. By default, the rule order between Ingresses within an IngressGroup is determined by the lexical order of Ingress\u2019s namespace/name. </p> <p>This default ordering of ingress rules can lead to application-wide incorrect routing if any of the Ingresses have misconfigured rules. Teams should create explicit ingress rules to avoid routing conflicts between multiple teams or applications and use <code>group.order</code> to set the order of execution. </p> <p>Here\u2019s another key consideration with using IngressGroup in a shared environment, The name of the IngressGroup is across the cluster as an IngressGroup can have Ingresses in multiple namespaces. Teams should avoid naming collisions by using unambiguous values for <code>group.name</code>. The AWS Load Balancer Controller currently doesn\u2019t offer fine grained controls to control access to an IngressGroup. So, avoid giving your group a generic name like \u201cMyIngressGroup\u201d, because someone else in the cluster may create an Ingress with the same name, which adds their Ingress to your group. If they create higher priority rules, they may highjack your application\u2019s traffic. </p> <p>AWS ALB (Application Load Balancer) has several limits on the number of rules that can be configured per listener. These limits are in place to prevent overloading the load balancer and impacting its performance. For e.g. Each ALB listener can have up to 100 rules by default. It's important to check the AWS documentation for the latest information on these limits.</p>"},{"location":"ALB-ingresssharing-tbg/#decouple-load-balancers-and-kubernetes-resources-with-targetgroupbinding","title":"Decouple Load Balancers and Kubernetes resources with TargetGroupBinding","text":"<p>In the previous section, we described the operations the AWS Load Balancer performs when you create an Ingress. The controller creates an ALB when a new Ingress resource is created. If an Ingress is part of IngressGroup, the controller merges ingress rules across Ingresses and configures the ALB, listeners, and rules. The lifecycle of the Load Balancer is tied to the associated one or more Ingresses. If you delete Ingress, the AWS Load Balancer Controller will delete the ALB given there are no other Ingresses in the group. The controller creates and configures load balancers to route traffic to your applications. </p> <p>There are a few scenarios in which customers prefer managing a load balancer themselves. They separate the creation and deletion load balancers from the lifecycle of a Service or Ingress. We have worked with customers that do not give EKS clusters the permission to create load balancers. In other situations, teams wanted to migrate workloads to EKS but wanted to preserve the load balancer. In both scenarios, teams needed to the ability to use a  pre-existing load balancer to expose Kubernetes Services. </p> <p><code>TargetGroupBinding</code> is a custom resource managed by the AWS Load Balancer Controller. It allows you to expose  Kubernetes applications using existing load balancers. A <code>TargetGroupBinding</code> resource binds a Kubernetes Service with a load balancer target group. When you create a <code>TargetGroupBinding</code> resource, the controller automatically configures the target group to route traffic to a Service.  Here\u2019s an example of a TargetGroupBinding resource:  Example of a TargetGroupBinding resource</p> <p>An obvious advantage is that the load balancer remains static as you create and delete Ingresses or even clusters. The lifecycle of the load balancer becomes independent from the Service(s) its exposing. An even bigger benefit is that now you can use an pre-existing ALB to distribute traffic to multiple EKS clusters. </p>"},{"location":"ALB-ingresssharing-tbg/#load-balance-application-traffic-across-clusters","title":"Load balance application traffic across clusters","text":"<p>ALB can distribute traffic to multiple backends using weighted target groups. You can use this feature to route traffic to multiple clusters by first creating a target group for each cluster and then binding the target group to Services in multiple clusters. This strategy allows you to control the percentage of traffic you send to each cluster. </p> <p>Such traffic controls are especially useful when performing blue/green cluster upgrades. You can migrate traffic from the older cluster to the newer in a controlled manner.   Load balance application traffic across clusters</p> <p>Customers also use this architecture to improve workload resilience in multi-cluster environments. There are customers that deploy their applications to multiple Kubernetes clusters simultaneously. By doing this, they eliminate any Kubernetes cluster from becoming a single point of failure. In case one cluster experiences disrupting events, you can remove it from load balancer targets. </p>"},{"location":"ALB-ingresssharing-tbg/#walkthrough","title":"Walkthrough","text":"<p>To demonstrate how TargetGroupBinding works in action, we\u2019re going to deploy two versions of a web application in an EKS cluster. We\u2019ll name the deployments \u201cblack\u201d and \u201cred\u201d. Both applications will run in their dedicated namespaces. We\u2019ll distribute traffic evenly between the two replicas of our application using an ALB. </p> <p>We\u2019ll create two Services to expose the applications. We will then associate these Services with target groups by creating TargetGroupBinding resources. </p> <p>Pre-requisite: To be able to follow along, you\u2019ll need:</p> <ul> <li>An EKS cluster with AWS Load Balancer Controller installed</li> <li>Ability to create Application Load Balancer, target groups, and listener</li> </ul>"},{"location":"ALB-ingresssharing-tbg/#setup-load-balancing","title":"Setup Load Balancing","text":"<p>Set the IDs of your VPC and public subnets in environment variables:</p> <pre><code>VPC_ID=vpc-id\nPUB_SUBNET_IDS=$(aws ec2 describe-subnets --filter Name=vpc-id,Values=$VPC_ID --query 'Subnets[?MapPublicIpOnLaunch==`true`].SubnetId')\n</code></pre> <p>Create an Application Load Balancer: </p> <pre><code>ALB_ARN=$(aws elbv2 create-load-balancer \\\n  --name TargetGroupBinding-Demo-ALB\\\n  --type application \\\n  --subnets ${PUB_SUBNET_IDS} \\\n  --query 'LoadBalancers[].LoadBalancerArn' \\\n  --output text)\n</code></pre> <p>Create a listener and two target groups in the same VPC as your EKS cluster:</p> <pre><code>`RED_TG``=``$``(``aws elbv2 create``-``target``-``group`` \\`\n`  ``--``name ``my``-``service``-``red \\`\n`  ``--``port ``80`` \\`\n`  ``--``protocol HTTP \\`\n`  ``--``target``-``type ip \\`\n`  ``--``vpc``-``id $VPC_ID \\`\n`  ``--``query ``'TargetGroups[].TargetGroupArn'`` \\`\n`  ``--``output text``)\n  `\n`BLACK_TG``=``$``(``aws elbv2 create``-``target``-``group`` \\`\n`  ``--``name ``my``-``service``-``black \\`\n`  ``--``port ``80`` \\`\n`  ``--``protocol HTTP \\`\n`  ``--``target``-``type ip \\`\n`  ``--``vpc``-``id $VPC_ID \\`\n`  ``--``query ``'TargetGroups[].TargetGroupArn'`` \\`\n`  ``--``output text``)`\n`  `\nLISTENER_ARN=$(aws elbv2 create-listener \\\n   --load-balancer-arn $ALB_ARN \\\n   --protocol HTTP \\\n   --port 80 \\\n   --default-actions \\\n   '[{\"Type\": \"forward\", \n   \"Order\": 1, \n   \"ForwardConfig\": \n     {\"TargetGroups\": [\n     {\"TargetGroupArn\": \"'${RED_TG}'\", \"Weight\": 50},\n     {\"TargetGroupArn\": \"'${BLACK_TG}'\", \"Weight\": 50}\n     ]\n   }\n   }]' \\\n   --query 'Listeners[].ListenerArn' \\\n   --output text)\n</code></pre> <p>Here\u2019s a screenshot of the target group rules in AWS Management Console after applying the configuration: </p>"},{"location":"ALB-ingresssharing-tbg/#deploy-applications","title":"Deploy applications","text":"<p>Now that we have an Application Load Balancer and the two target groups created, we will need to associate the two target groups with corresponding Services. Let\u2019s create manifests for the two target group binding CRDs:</p> <pre><code># Red Service target group binding\ncat &gt; containers-blog-maelstrom/aws-lb-controller-blog/target-grp-binding/red-app-tgb.yaml &lt;&lt; EOF\napiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: red-tgb\n  namespace: red-ns\nspec:\n  serviceRef:\n    name: red-service\n    port: 80\n  targetGroupARN: ${RED_TG}\nEOF\n\n# Black Service target group binding\ncat &gt; containers-blog-maelstrom/aws-lb-controller-blog/target-grp-binding/black-app-tgb.yaml &lt;&lt; EOF\n apiVersion: elbv2.k8s.aws/v1beta1\n kind: TargetGroupBinding\n metadata:\n   name: black-tgb\n   namespace: black-ns\n spec:\n   serviceRef:\n     name: black-service\n     port: 80\n   targetGroupARN: ${BLACK_TG}\nEOF\n</code></pre> <p>Next, create the application and target group bindings in your cluster:</p> <pre><code>kubectl apply -f containers-blog-maelstrom/aws-lb-controller-blog/target-grp-binding/\n</code></pre> <p>Let\u2019s switch back to the AWS Management Console to visualize this configuration. Navigate to either of the target groups and you\u2019ll see that the AWS Load Balancer Controller has registered corresponding Pods as targets  Once the targets are in <code>healthy</code> status, navigate to the DNS name of the ALB and open it in a browser. You may get a page with a black or blue background. Refresh the page and the colors should alternate. </p> <p>Note: if you receive a timeout error when accessing the page, verify that the ALB\u2019s security groups have an inbound rule to permit HTTP traffic from your IP address.  </p> <p>If you get pages with alternating background, ALB is forwarding your requests to the two Services running in their respective namespace. We can even move <code>/black</code> service to another cluster to load balance traffic between multiple EKS clusters. </p>"},{"location":"ALB-ingresssharing-tbg/#clean-up","title":"Clean up","text":"<p>Delete the resources created in this post:</p> <pre><code>kubectl delete -f containers-blog-maelstrom/aws-lb-controller-blog/ingress-grouping/\nkubectl delete -f containers-blog-maelstrom/aws-lb-controller-blog/target-grp-binding/\naws elbv2 delete-load-balancer --load-balancer-arn $ALB_ARN\naws elbv2 delete-listener --listener-arn $LISTENER_ARN\naws elbv2 delete-target-group --target-group-arn $RED_TG\naws elbv2 delete-target-group --target-group-arn $BLACK_TG\n</code></pre>"},{"location":"ALB-ingresssharing-tbg/#conclusion","title":"Conclusion","text":"<p>In this blog we showed how to save costs by sharing an Application Load Balancer with multiple Ingress resources. We also explored how to decouple the lifecycle of load balancers from that of Service and Ingress resources in a Kubernetes cluster. Finally, we learned how you can use ALB weighted target groups to route traffic to multiple clusters using AWS Load Balancer Controller\u2019s TargetGroupBinding feature.</p> <p>For more information, see the following references:</p> <ul> <li>AWS Load Balancer Controller LiveDoc</li> <li>Different Annotations supported</li> <li>Github Link for the AWS Load Balancer Controller project</li> <li>Previous Blog on AWS Load Balancer Controller</li> </ul> <p></p>"},{"location":"ALB-ingresssharing-tbg/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"ALB-ingresssharing-tbg/#re-alvarez-parmar","title":"Re Alvarez Parmar","text":"<p>In his role as Containers Specialist Solutions Architect at Amazon Web Services. Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent over 15 years as Enterprise and Software Architect. He is based out of Seattle. You can connect with him on LinkedIn linkedin.com/in/realvarez/</p> <p></p>"},{"location":"ALB-ingresssharing-tbg/#ratnopam-chakrabarti","title":"Ratnopam Chakrabarti","text":"<p>Ratnopam Chakrabarti is a Specialist Solutions Architect for Containers and App modernization at Amazon Web Services (AWS). In his current role, Ratnopam helps customers in accelerating their cloud adoption by leveraging AWS container technologies at scale. He is based out of Dallas, Texas. You can connect with him on LinkedIn https://www.linkedin.com/in/ratnopam-chakrabarti/.</p> <p></p>"},{"location":"ALB-ingresssharing-tbg/#praseeda-sathaye","title":"Praseeda Sathaye","text":"<p>Praseeda Sathaye is a Principal Specialist SA for App Modernization and Containers at Amazon Web Services based in Bay Area California. She has been focused on helping customers speed their cloud-native adoption journey by modernizing their platform infrastructure, internal architecture using Microservices Strategy, Containerization, DevOps, Service Mesh, and Cloud Migration. At AWS she is working on AWS services like EKS, App Mesh, ECS and helping customers to run at scale.</p>"},{"location":"batch-processing-with-k8s/","title":"Batch processing in scale using K8s jobs and step functions","text":"<p>This article talks about an approach that can help customers scale large file processing workloads in AWS EKS using Kubernetes jobs and AWS step functions.</p>"},{"location":"batch-processing-with-k8s/#introduction","title":"Introduction","text":"<p>This approach uses AWS step functions to orchestrate the end to end flow, which involves:</p> <ul> <li>Reading of the input file from AWS S3</li> <li>Splitting the large input file into smaller files, processing it, saving the data into a database</li> <li>Writing the output file to AWS S3</li> </ul> <p>Before we get into the implementation details, let us discuss the alternative approach to address the same use case.</p> <ul> <li>Sequential read (aka single-threaded) - This approach uses a simple file reader that reads the large file row by row and processes it.</li> <li>Big data system (using spark) - This approach is quite popular and commonly implemented when it comes to processing large files. It uses spark api's like <code>spark.csv.read()</code> to read the file, convert them into dataframes or RDD, process them and write the data back into to a data lake as CSV, txt or parquet files</li> <li>Stream-based system (using message bus) - In this approach, a program reads the file row by row and pushes the records as individual messages to a message bus (like <code>Kafka</code> or <code>AWS Kinesis streams</code>). A consumer (like AWS Lambda)  reads the message, processes it, and stores the result in a file system or database</li> </ul> <p>Note: Let us assume a large file, in this case, refers to a file with at least one million records, delimited by line breaks (with each row presenting a single record)</p> <p>Here is a relative comparison of these approaches:</p> Name Description Sequential read (#1) Big data system (#2) Stream-based system (#3) Row order Can the output file be generated in the same row order as input file Yes No No Horizontal scaling Will horizontal scaling help? No Yes Yes Vertical scaling Will vertical scaling help? Yes Yes Yes Processing time Time taken to process a large file Relatively slow Faster than option #1 Faster than option #1 Level of refactor Percentage of refactoring required in moving an existing codebase 30% to 40% 80% to 90% 60% to 70% <p>Each of these approaches has its advantages (like scaling, elasticity) and disadvantages (like maintaining row order, level of refactoring involved) when comparing with one another. So part of this article, let us see how we can implement an hybrid approach that can provide scalability, elasticity, and still maintain the row order in the output file with an acceptable (30% to 40%) level of refactoring.</p>"},{"location":"batch-processing-with-k8s/#solution-overview","title":"Solution overview","text":""},{"location":"batch-processing-with-k8s/#aws-services","title":"AWS Services","text":"<p><code>AWS Step function</code> and <code>Kubernetes Job</code> are the two essential technologies used to implement this approach, and here is a high level overview of them.</p>"},{"location":"batch-processing-with-k8s/#aws-step-function","title":"AWS Step function","text":"<p>AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that maintain the application state. Read more about it here</p>"},{"location":"batch-processing-with-k8s/#kubernetes-job","title":"Kubernetes Job","text":"<p>A Job creates one or more Pods and will continue to retry execution of the Pods until a specified number of them successfully terminate. As pods complete, the job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, job) is done. Read more about it here</p>"},{"location":"batch-processing-with-k8s/#sample-application","title":"Sample application","text":"<p>To demonstrate this approach, we have implemented a sample application that can take in an input file, process it, save the data in <code>Dynamodb</code> and write the output file to <code>AWS S3</code>. The below image shows the high-level architecture of the application:</p> <p></p> <p>Here is the end to end flow of the application:</p> <p><code>AWS Step function</code> is invoked when the input file gets dropped into the <code>AWS S3</code> bucket. <code>AWS Cloudtrail</code> listens to all the write events of the input S3 bucket. <code>AWS Step functions</code> will execute the below steps, part of file processing:</p> <ol> <li><code>File splitter</code> - Kubernetes job will read the input file from S3.</li> <li><code>File splitter</code> will split the large input file into smaller files and writes them to the <code>Elastic file system</code> mounted on the pod. The program uses the Unix <code>split</code> command to chunk the large files into smaller ones, with each file containing a maximum of <code>MAX_LINES_PER_BATCH</code> lines, set in the environment variable, defaults to 30000.</li> <li>Save the path of the split files in <code>AWS Elastic cache</code> (Redis) that will get used in tracking the overall progress of this job. The data in Redis cache gets stored in this format:     | Format  | Data type | Sample data |     | ----- | ------------- | ----- |     |  | &lt;string, set\\&lt;string&gt;&gt; | &lt;CloudTrail_Event_Id, Set\\&lt;\\&lt;EFS_Path_Of_SplitFiles&gt;&gt; | <li><code>Split-file-lambda</code> - Lambda function will read the Redis cache and return an array of split file locations as response</li> <li><code>Map state</code> will use the split files array as input and create parallel Kubernetes jobs to process all these split files in parallel, with an <code>MaxConcurrency = 0</code>. Each job will receive one split file as input and takes care of the following:<ul> <li>Read the split file from the EFS location</li> <li>Process each row, generate <code>ConfirmationId</code> for <code>OrderId</code> field available in the input. Save the information in <code>AWS Dynamodb</code> under <code>Orders</code> table. All dynamodb writes are batched to a maximum of 25 rows per request.</li> <li>Part of this process, a CSV file gets created in EFS location with each row containing both <code>ConfirmationId</code> and <code>OrderId</code>, written in batch</li> <li>Updates elastic cache by removing <code>split file (path)</code> from Redis set using <code>rdb.SRem</code> command</li> <li>If the set associated with the <code>CloudTrail_Event_Id</code> is empty, then all the parallel file processing jobs are complete. So we can merge the output split files in the EFS directory and upload them to the S3 bucket</li> </ul> </li> <p>Note: It\u2019s very important to settle on a right value for the maximum number of rows a split input file can contain. We set this value via <code>MAX_LINES_PER_BATCH</code> environment variable. Giving a smaller value will end up with too many split files causing too many containers to get created, and setting a large value will leaves too little scope for parallelism.</p> <p>Below are the snapshots of various artifacts used in this flow</p> <p>Input file:</p> Region Country Item Type Sales Channel Order Priority Order Date Order ID Ship Date Units Sold Unit Price Unit Cost Total Revenue Total Cost Total Profit Sub-Saharan Africa South Africa Fruits Offline M 7/27/2012 791862618 7/28/2012 1593 9.33 6.92 14862.69 11023.56 3839.13 <p>Output file:</p> <p>791862618,aefa3585-7f46-476e-bc22-70181704c240</p> <p>Orders (Dynamodb table):</p> Region Country Item Type Sales Channel Order Priority Order Date Order ID Ship Date Units Sold Unit Price Unit Cost Total Revenue Total Cost Total Profit ConfirmationId Sub-Saharan Africa South Africa Fruits Offline M 7/27/2012 791862618 7/28/2012 1593 9.33 6.92 14862.69 11023.56 3839.13 aefa3585-7f46-476e-bc22-70181704c240"},{"location":"batch-processing-with-k8s/#build-and-deployment","title":"Build and Deployment","text":""},{"location":"batch-processing-with-k8s/#pre-requistes","title":"Pre-requistes","text":"<ul> <li>AWS CDK should be installed in the local laptop. You can read more about it here</li> <li><code>Yarn</code> needs to be installed, you can check the installation status by running this command</li> <li>An AWS account with console and API access</li> <li>Docker desktop needs to be installed in the local laptop, you can read more about it here</li> </ul> <pre><code>yarn version\n</code></pre> <p>Output 1.22.10</p> <p>If <code>Yarn</code> is not installed, run the following command</p> <pre><code>npm install -g yarn\n</code></pre>"},{"location":"batch-processing-with-k8s/#build","title":"Build","text":"<p>Check out the code from this repository using this command:</p> <pre><code>mkdir batch-processing-with-k8s &amp;&amp; cd batch-processing-with-k8s\ngit clone git@ssh.gitlab.aws.dev:am3-app-modernization-gsp/eks/batch-processing-with-k8s.git .\n</code></pre> <p>Note: Source code for all Kubernetes jobs and lambda functions are available under src folder</p>"},{"location":"batch-processing-with-k8s/#deploy","title":"Deploy","text":"<p>Code for the sample application using this CDK construct is available in <code>src/integ.default.ts</code>. In order to deploy the application, first bootstrap a CDK environment (if you haven't done so already).</p> <pre><code># Bootstrap CDK (ONLY ONCE, if you have already done this you can skip this part)\n# Subsitute your AWS Account Id and AWS region in the command below\ncdk bootstrap \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\naws://&lt;AWS Account Id&gt;/&lt;AWS_REGION&gt;\n</code></pre> <p>The code is created as a CDK construct, the following parameters can be customized as part of the deployment</p> Parameter Description Default vpc VPC in which the resources needs to be created New VPC will be created minNodes Autoscaling parameter for minimum value for EKS worker nodes 5 desiredNodes Autoscaling parameter for desired value for EKS worker nodes 5 maxNodes Autoscaling parameter for maximum value for EKS worker nodes 5 inputBucket S3 bucket, where the input files will get dropped input-bucket maxSplitLines Maximum number of records per split input file 30000 <p>Run the following command to start the deployment</p> <pre><code>cdk deploy --require-approval never\n</code></pre> <p>Once the deployment is successful, you will see the following output:</p> <pre><code> \u2705  file-batch-stack\n\nOutputs:\nfile-batch-stack.KubernetesFileBatchConstructInputBucketName610D8598 = file-batch-stack-kubernetesfilebatchconstructinpu-1u3xbu9ycgorp\nfile-batch-stack.KubernetesFileBatchConstructMultithreadedstepfuctionF3358A99 = KubernetesFileBatchConstructfilebatchmultithreaded0B80AF5A-abBRhxEtxLig\nfile-batch-stack.KubernetesFileBatchConstructfilebatchEFSFileSystemId9139F216 = fs-696fb5dd\nfile-batch-stack.KubernetesFileBatchConstructfilebatcheksclusterClusterName146E1BCB = KubernetesFileBatchConstructfilebatchekscluster6B334C7D-7874a48b84604e20a0dc68ecd3715e27\nfile-batch-stack.KubernetesFileBatchConstructfilebatcheksclusterConfigCommand3063A155 = aws eks update-kubeconfig --name KubernetesFileBatchConstructfilebatchekscluster6B334C7D-7874a48b84604e20a0dc68ecd3715e27 --region us-east-1 --role-arn arn:aws:iam::775492342640:role/file-batch-stack-KubernetesFileBatchConstructfileb-146I0AN7L7JXW\nfile-batch-stack.KubernetesFileBatchConstructfilebatcheksclusterGetTokenCommandAD6928E0 = aws eks get-token --cluster-name KubernetesFileBatchConstructfilebatchekscluster6B334C7D-7874a48b84604e20a0dc68ecd3715e27 --region us-east-1 --role-arn arn:aws:iam::775492342640:role/file-batch-stack-KubernetesFileBatchConstructfileb-146I0AN7L7JXW\nfile-batch-stack.KubernetesFileBatchConstructfilebatcheksclusterMastersRoleArn52BC348E = arn:aws:iam::775492342640:role/file-batch-stack-KubernetesFileBatchConstructfileb-146I0AN7L7JXW\n\nStack ARN:\narn:aws:cloudformation:us-east-1:775492342640:stack/file-batch-stack/886208f0-aeb2-11eb-9592-0e4dccb471bf\n</code></pre> <p>Note: Make sure to run the <code>ClusterConfig</code> command available part of the CDK output. CDK script will add the newly created AWS EKS cluster to the kubeconfig to run kubectl command using this.</p> <p>The deployment will take care of building the docker images for all the k8s jobs, lambda functions and uploading it to <code>AWS ECR</code></p>"},{"location":"batch-processing-with-k8s/#testing","title":"Testing","text":""},{"location":"batch-processing-with-k8s/#unit-testing","title":"Unit testing","text":"<p>Unit testcases can be executed by running the following command from the root directory</p> <pre><code>yarn test\n\nyarn run v1.22.10\n$ npx projen test\n\ud83e\udd16 test | rm -fr lib/\n\ud83e\udd16 test \u00bb test:compile | tsc --noEmit --project tsconfig.jest.json\n\ud83e\udd16 test | jest --passWithNoTests --all --updateSnapshot\n PASS  test/index.test.ts (7.722 s)\n\u2713 create app (3181 ms)\n\n----------|---------|----------|---------|---------|-------------------\nFile      | % Stmts | % Branch | % Funcs | % Lines | Uncovered Line #s\n----------|---------|----------|---------|---------|-------------------\nAll files |   95.95 |       75 |   85.71 |   95.95 |\nindex.ts |   95.95 |       75 |   85.71 |   95.95 | 558-642\n----------|---------|----------|---------|---------|-------------------\nTest Suites: 1 passed, 1 total\nTests:       1 passed, 1 total\nSnapshots:   0 total\nTime:        8.56 s\nRan all test suites.\n\ud83e\udd16 test \u00bb eslint | eslint --ext .ts,.tsx --fix --no-error-on-unmatched-pattern src test build-tools .projenrc.js\n\u2728  Done in 24.16s.\n</code></pre>"},{"location":"batch-processing-with-k8s/#integration-test","title":"Integration test","text":"<p>Lets run a simple end to end integration test using a simple input file test.csv under payload folder. Run the following command from the root directory:</p> <pre><code>&gt; aws s3api put-object --bucket &lt;&lt;input_bucket&gt;&gt; --key test.csv --body payload/test.csv\n</code></pre> <p>Note: Replace <code>input_bucket</code> with the actual input bucket available part of the CDK output</p> <p>Output</p> <pre><code>{\n\"ETag\": \"\\\"3ff3c83af2553279cd0f6f8fcf59980a\\\"\"\n}\n</code></pre> <p>Now the step function should get automatically triggered and see the updates in the execution details page. Navigate to the state machine homepage and select the state machine created by the CDK (available part of the CDK output). </p> <p>The step function should successfully get executed, and it should look like below in the execution details page:</p> <p></p> <p>The response file can be downloaded by running the following command:</p> <pre><code>aws s3 cp s3://&lt;&lt;input_bucket&gt;&gt;/test.csv_Output .\n</code></pre> <p>Note: Replace <code>input_bucket</code> with the actual input bucket available part of the CDK output</p> <p>Output</p> <pre><code>download: s3://&lt;&lt;input_bucket&gt;&gt;/test.csv_Single_Output to ./test.csv_Single_Output\n</code></pre>"},{"location":"batch-processing-with-k8s/#performance-testing","title":"Performance testing","text":"<ul> <li>Let\u2019s run some performance tests with different datasets varying from 200 rows to 2 million rows and see how the system behaves in scaling up and processing these files quicker.</li> <li>To represent the \"Sequential read (aka single-threaded)\" approach, we created a step function with just one step calling Kubernetes job, which takes care of reading the input file, processing it, saving the data in dynamodb, and uploading the output file to AWS S3.</li> <li>Code to create this step function is available part of the CDK index.ts. To create this step function uncomment lines from 187 to 193, comment everything from 178 to 184 and then run <code>cdk deploy --require-approval never</code>. This will delete the multi-threaded step function (enabled with map construct) and create the single-threaded one</li> </ul>"},{"location":"batch-processing-with-k8s/#results","title":"Results","text":"File Records Sequential read Parallel (map) test.csv 200 68 seconds 117 seconds data.csv 1 Million 545 seconds 189 seconds data2M.csv 2 Million 1194 seconds 243 seconds <p>Once we plot the numbers in a chart, here is how it looks like:</p> <p></p> <p>As the load grows, we can see the \"Parallel (map)\" approach scales better compared to the \"Sequential read\" model.</p> <p>Note: All the payloads used in running these tests are available part of the payload folder in the root directory</p> <p>Here is how varying the value of <code>MAX_LINES_PER_BATCH</code> environment variable can fluctuate the overall performance of the system.</p> <p></p> <p>As highlighted above just setting the smallest value to <code>MAX_LINES_PER_BATCH</code> variable doesn't make the system to run automatically faster. As we have set <code>MaxConcurrency to zero</code> in the map state, AWS step function will try to provide maximum parallelism by running as many k8s jobs as possible in parallel. So all the k8s jobs will start competing for the same hardware resources (EKS worker nodes) which will make concurrency more a bottleneck rather than a value add. We can overcome this behavior by setting the right value to <code>MaxConcurrency</code> attribute and enabling autoscaling for EKS worker nodes. Scaling EKS worker are faster but they are not instantaneous so the whole scaling process by itself will definitely take sometime, eventually causing delays in the processing of workload.</p> <p>So its definitely essential to run some experiments and decide on right values for both <code>MAX_LINES_PER_BATCH</code> environment variable and <code>MaxConcurrency</code> attribute in Map state</p>"},{"location":"batch-processing-with-k8s/#cleanup","title":"Cleanup","text":"<p>Run the following command from the root directory to delete the stack</p> <pre><code>cdk destroy\n</code></pre>"},{"location":"batch-processing-with-k8s/#resources","title":"Resources","text":"<ul> <li>AWS Step functions</li> <li>Kubernetes jobs</li> <li>Amazon EKS with Step function</li> <li>CSI Driver EFS</li> <li>Unix split command</li> <li>Redis set commands</li> </ul>"},{"location":"batch-processing-with-k8s/#hari-ohm-prasath","title":"Hari Ohm Prasath","text":"<p>Hari is a Senior Software Development Engineer with App Runner and Elastic Beanstalk, working on new ways for customers to modernize their web applications on AWS. Hari loves to code and actively contributes to the open source initiatives. You can find him in Medium, Github &amp; Twitter @hariohmprasath</p> <p></p>"},{"location":"batch-processing-with-k8s/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"batch-processing-with-k8s/#re-alvarez-parmar","title":"Re Alvarez Parmar","text":"<p>In his role as Containers Specialist Solutions Architect at Amazon Web Services. Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent over 15 years as Enterprise and Software Architect. He is based out of Seattle. You can connect with him on LinkedIn linkedin.com/in/realvarez/</p>"},{"location":"capture-errors-k8s-slack/","title":"Using Amazon EKS Blueprints and Open Telemetry to capture errors from Kubernetes applications on Slack","text":""},{"location":"capture-errors-k8s-slack/#introduction","title":"Introduction","text":"<p>The Operational Excellence pillar of AWS Well-Architected Framework focusses on the the ability to support development and run workloads effectively, gain insight into their operations, and to continuously improve supporting processes and procedures to deliver business value. Capturing errors from your Kubernetes applications and effectively alerting those errors are very important to improve business processes to constantly deliver real business value to your end users. Efficient and effective management of these operational events is required to achieve operational excellence.</p> <p>In this blog we will using Amazon EKS Blueprints as a vehicle to create your Amazon EKS clusters with Day 2 operational tooling such as AWS Distro for Open Telemetry (ADOT) and ADOT CloudWatch Collector to capture errors from your Kubernetes application. The solution will then deploy a AWS Serverless Application Model (SAM) template which will generate CloudWatch alarms for errors from your Kubernetes application and invokes an Amazon Lambda function to send real time alerts to Slack. </p>"},{"location":"capture-errors-k8s-slack/#solution-overview","title":"Solution Overview","text":"<p>Here is a functional flow of this solution:</p> <ol> <li>AWS Distro for Open Telemetry (ADOT) Operator manages the life cycle of your ADOT CloudWatch Collector. </li> <li>ADOT CloudWatch Collector scrapes metrics from application and  sends metrics to Amazon CloudWatch.</li> <li>Amazon CloudWatch custom metric filters monitors required events in respective CloudWatch log groups.</li> <li>CloudWatch Alarm triggers notification to Amazon SNS when a threshold is breached.</li> <li>Amazon SNS invokes an Amazon Lambda function which in-turn sends CloudWatch alarm notifications to Slack.</li> </ol>"},{"location":"capture-errors-k8s-slack/#prerequisites","title":"Prerequisites","text":"<p>Install the following utilities on a Linux based host machine, which can be an Amazon EC2 instance, Cloud9 instance or a local machine with access to your AWS account:</p> <ul> <li>AWS CLI version 2 to interact with AWS services using CLI commands</li> <li>Node.js (v16.0.0 or later) and npm (8.10.0 or later)</li> <li>AWS CDK v2.62.0or later to build and deploy cloud infrastructure and Kubernetes resources programmatically</li> <li>AWS SAM CLI to deploy AWS Lambda function</li> <li>Docker CLI to build docker images</li> <li>Kubectl to communicate with the Kubernetes API server</li> <li>Git to clone required source repository from GitHub</li> </ul> <p>Let\u2019s start by setting a few environment variables:</p> <pre><code>export CAP_ACCOUNT_ID=$(aws sts get-caller-identity \\\n--query 'Account' --output text)\nexport CAP_CLUSTER_REGION=\"us-east-2\"\nexport CAP_CLUSTER_NAME=\"demo-cluster\"\nexport CAP_FUNCTION_NAME=\"cloudwatch-to-slack\"\n</code></pre> <p>Clone the sample repository which contains the code for our solution :</p> <pre><code>git clone https://github.com/aws-samples/containers-blog-maelstrom.git\ncd ./containers-blog-maelstrom/aws-cdk-eks-app-alarms-to-slack\n</code></pre>"},{"location":"capture-errors-k8s-slack/#bootstrap-the-environment","title":"Bootstrap the Environment","text":"<p>In this solution we will be using Amazon EKS CDK Blueprints to provision our Amazon EKS cluster. The first step to any CDK deployment is bootstrapping the environment. <code>cdk bootstrap</code> is a tool in the AWS CDK command-line interface (AWS CLI) responsible for preparing the environment (i.e., a combination of AWS account and AWS Region) with resources required by CDK to perform deployments into that environment. If you already use CDK in a region, you don\u2019t need to repeat the bootstrapping process. </p> <p>Lets run the below script to bootstrap your environment and install all node dependencies required for deploying the solution:</p> <pre><code>sh ./bootstrap-env.sh\n</code></pre> <p>Please navigate to <code>bin/cluster-blueprint.ts</code> in the cloned repo to check on the Amazon EKS CDK Blueprints stack which will deploy EKS Cluster with day 2 operational add-ons required to run our solution. Please see the below <code>bin/cluster-blueprint.ts</code> snippet showing our EKS CDK Blueprints stack :</p> <pre><code>import 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\n\nconst app = new cdk.App();\n\nconst account = process.env.CAP_ACCOUNT_ID! || process.env.CDK_DEFAULT_ACCOUNT!;\nconst region = process.env.CAP_CLUSTER_REGION! || process.env.CDK_DEFAULT_REGION!;\nconst clusterName = process.env.CAP_CLUSTER_NAME!;\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    new blueprints.addons.VpcCniAddOn(),\n    new blueprints.addons.CoreDnsAddOn(),\n    new blueprints.addons.KubeProxyAddOn(),\n    new blueprints.addons.CertManagerAddOn(),\n    new blueprints.addons.AdotCollectorAddOn(),\n    new blueprints.addons.CloudWatchAdotAddOn({deploymentMode: blueprints.addons.cloudWatchDeploymentMode.DEPLOYMENT,\n        metricsNameSelectors: ['apiserver_request_.*', 'container_memory_.*', 'container_threads', 'otelcol_process_.*', 'ho11y*'],\n        podLabelRegex: 'frontend|downstream(.*)'}\n    )\n];\n\nconst stack = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .addOns(...addOns)\n    .build(app, clusterName);\n</code></pre> <p>Next, run the <code>cdk list</code> command which lists name of stack that will be created.</p> <pre><code>cdk list\n</code></pre> <p>If you are interested in knowing list of resources that will be created by this stack, you can view them using <code>cdk diff</code> command. </p>"},{"location":"capture-errors-k8s-slack/#create-the-clusters","title":"Create the clusters","text":"<p>Run below command to deploy the Amazon EKS cluster with day 2 operational add-ons required to run our solution. </p> <pre><code>cdk deploy \"*\" --require-approval never\n</code></pre> <p>Deployment will take approximately 20-30 minutes to complete. Upon completion, you will have a fully functioning EKS cluster deployed in your account. </p> <p></p> <p>Please copy and run the <code>aws eks update-kubeconfig...</code> command as shown in the above screenshot to gain access to your Amazon EKS cluster using <code>kubectl</code>.</p>"},{"location":"capture-errors-k8s-slack/#aws-distro-for-opentelemetry-adot-operator","title":"AWS Distro for OpenTelemetry (ADOT) Operator","text":"<p>AWS Distro for OpenTelemetry (ADOT) is a production ready, secure, AWS-supported distribution OpenTelemetry project. With ADOT, you can instrument your application once and send correlated metrics and traces to many monitoring solutions including Amazon CloudWatch. You can deploy ADOT in EKS as an add-on which in-turn deploys ADOT Operator. ADOT Operator manages the lifecyle of your ADOT Collector, a collection agent which receives, processes, and exports telemetry data. With the deployment in the previous step, ADOT Operator is deployed as an add-on (AdotCollectorAddOn) in the namespace <code>opentelemetry-operator-system</code>. </p> <p>Run the below command to check the running <code>opentelemetry-operator-controller-manager</code> pods :</p> <pre><code>kubectl get po -n opentelemetry-operator-system \\\n-l app.kubernetes.io/name=opentelemetry-operator\n\nNAME                                                         READY   STATUS    RESTARTS   AGE\nopentelemetry-operator-controller-manager-76b94f4756-lgq69   2/2     Running   0          7m36s\n</code></pre>"},{"location":"capture-errors-k8s-slack/#adot-cloudwatch-collector","title":"ADOT CloudWatch Collector","text":"<p>To send application metrics and traces to Amazon CloudWatch, you can use Amazon CloudWatch ADOT Collector add-on. You can deploy this add-on as Deployment, Daemonset, StatefulSet or Sidecar as per your deployment strategy. Amazon CloudWatch ADOT Collector blueprints add-on is deployed with filters on specific metrics and pod labels using <code>metricsNameSelectors</code> and <code>podLabelRegex</code>. </p> <p>Run the below command to validate installation of CloudWatch ADOT Collector add-on and this shows the list of collector pods :</p> <pre><code>kubectl get po -n default \\\n-l app.kubernetes.io/component=opentelemetry-collector\n\nNAME                                        READY   STATUS    RESTARTS   AGE\notel-collector-cloudwatch-collector-x5djc   1/1     Running   0          7m43s\n</code></pre>"},{"location":"capture-errors-k8s-slack/#deploy-sample-application-ho11y","title":"Deploy Sample Application (ho11y)","text":"<p>Next, let us deploy a sample application called ho11y, a synthetic signal generator that lets you test observability solutions for microservices. It emits logs, metrics, and traces in a configurable manner. For more information, see the AWS O11y Receipes respository. </p> <p>Run the below command to deploy ho11y application:</p> <pre><code>kubectl apply -f ./templates/ho11y-app.yaml\n</code></pre> <p>Once <code>ho11y</code> app is deployed successfully, run the the below command to list all pods in <code>ho11y</code> namespace on your EKS cluster.</p> <pre><code>kubectl get po -n ho11y\n\nNAME                           READY   STATUS    RESTARTS   AGE\ndownstream0-5d48fd95b6-xl28g   1/1     Running   0          23s\ndownstream1-79ccd4b64c-fbxnv   1/1     Running   0          23s\nfrontend-7758b97475-lvfqk      1/1     Running   0          24s\n</code></pre>"},{"location":"capture-errors-k8s-slack/#create-cloudwatch-metrics","title":"Create CloudWatch Metrics","text":"<p>You can convert log data into numerical CloudWatch metrics using metric filters. Metric filters allow you to configure rules to extract metric data from log events. <code>ho11y</code> app exposes number of metrics via <code>/metrics</code>endpoint. One such request based metric <code>ho11y_total</code> is used in the below <code>put-metric-filter</code> command to create metric filter with dimensions such as <code>namespace</code> and <code>http_status_code</code>.</p> <p>Lets run the below commands to create metric filter:</p> <pre><code>aws logs put-metric-filter --region ${CAP_CLUSTER_REGION} \\\n--log-group-name /aws/containerinsights/${CAP_CLUSTER_NAME}/prometheus \\\n--cli-input-json file://templates/ho11y-metric-filter.json\n</code></pre>"},{"location":"capture-errors-k8s-slack/#send-cloudwatch-alarms-to-slack","title":"Send CloudWatch alarms to Slack","text":"<p>Next, lets configure our solution to send CloudWatch notification alarms to Slack. Amazon CloudWatch alarmscan be created for required metric and one or more actions can be triggered based on metric value relative to a threshold over a duration. One such action is to send notification to Simple Notification Service (SNS). We will then use Lambda function to process this notification and forward to Slack using incoming webhook. </p> <p>Create Slack Incoming Webhook</p> <p>Slack allows you to send messages from other applications using incoming webhook. Please refer to sending messages using incoming webhooks for more details. We will use this incoming webhook to send required CloudWatch alarms to Slack channel. Follow below steps to configure incoming webhook in Slack:</p> <ol> <li>Create or pick a Slack channel to send CloudWatch alarms notifications.</li> <li>Go to <code>https://&lt;your-team-domain&gt;.slack.com/services/new</code> and search for \"Incoming WebHooks\", select and click \"Add to Slack\".</li> <li>Under Post to Channel, choose the Slack channel where messages will be sent and click \"Add Incoming WebHooks Integration\".</li> <li>Copy webhook URL from the setup instructions and save it. This URL will be used in Lambda function.</li> </ol> <p>Create KMS Key</p> <p>In order to increase security posture of incoming webhook URL, we will now encrypt it using AWS KMS keys. Create KMS Key and key alias using below mentioned commands:</p> <pre><code>CAP_KMS_KEY_ID=$(aws kms create-key --region ${CAP_CLUSTER_REGION} \\\n--description \"Encryption Key for lambda function ${CAP_FUNCTION_NAME}\" \\\n--key-spec SYMMETRIC_DEFAULT --key-usage ENCRYPT_DECRYPT \\\n--query KeyMetadata.KeyId --output text)\n\naws kms create-alias --region ${CAP_CLUSTER_REGION} \\\n--alias-name alias/${CAP_FUNCTION_NAME}-key --target-key-id $CAP_KMS_KEY_ID\n</code></pre> <p>Create an Amazon Lambda function</p> <p>As next step, we will create Lambda function to send CloudWatch alarm notifications to Slack using AWS Serverless Application Model (SAM). AWS SAM uses AWS CloudFormation as the underlying deployment mechanism. We will create following resources using AWS SAM template:</p> <ol> <li>An Amazon SNS topic to which notifications will be published by CloudWatch alarm.</li> <li>A Lambda execution role to grant function permission with basic access and to decrypt using KMS Key.</li> <li>A Lambda function to send notifications to Slack using incoming webhook URL.</li> <li>Permission for SNS to trigger Lambda function.</li> </ol> <p>The script <code>deploy-sam-app.sh</code>  intakes the following two input values to deploy SAM template.</p> <ol> <li>Slack incoming webhook URL which you created previously. </li> <li>Slack channel name (you selected previously) to which notifications need to be sent</li> </ol> <p><code>deploy-sam-app.sh</code> script, Slack incoming webhook URL will be encrypted (client-side) using KMS Key to specific encryption context. Lambda function will decrypt using same encryption context. Lambda execution role is provided with fine-grained access to use KMS Key only for the specific encryption context.  Run the below command to deploy SAM template.</p> <pre><code>sh ./deploy-sam-app.sh\n</code></pre> <p>Test Lambda function</p> <p>Next, let us validate our Lambda function by pushing a test event using the payload available at <code>templates/test-event.json</code>. Run the command given below to push a test event:</p> <pre><code>aws lambda invoke --region ${CAP_CLUSTER_REGION} \\\n--function-name ${CAP_FUNCTION_NAME} \\\n--log-type Tail \\\n--query LogResult --output text \\\n--payload $(cat templates/test-event.json | base64 -w 0) -` \\\n| base64 -d`\n</code></pre> <p>Successful run will post test message to Slack channel and have command output as shown below :</p> <p></p> <p></p> <pre><code>[INFO]  2023-02-17T22:48:12.041Z        a07549ea-3dcf-4131-9f15-b676f461bbbd    Message posted to ho11y-cloudwatch-alarms\nEND RequestId: a07549ea-3dcf-4131-9f15-b676f461bbbd\nREPORT RequestId: a07549ea-3dcf-4131-9f15-b676f461bbbd  Duration: 292.01 ms     Billed Duration: 293 ms Memory Size: 128 MB     Max Memory Used: 69 MB  Init Duration: 399.38 ms\n</code></pre>"},{"location":"capture-errors-k8s-slack/#create-cloudwatch-alarms","title":"Create CloudWatch Alarms","text":"<p>Next, we will create CloudWatch alarm on a metric to create and send notifications to SNS topic. Below mentioned command creates CloudWatch alarm that monitors for http_status_code=400 and when the number of errors is above 15 in last 5 minutes, then a notification is sent to SNS topic.</p> <pre><code>SNS_TOPIC=$(aws cloudformation --region ${CAP_CLUSTER_REGION} describe-stacks --stack-name ${CAP_FUNCTION_NAME}-app --query 'Stacks[0].Outputs[?OutputKey==`CloudwatchToSlackTopicArn`].OutputValue' --output text)\n\naws cloudwatch put-metric-alarm --region ${CAP_CLUSTER_REGION} \\\n--alarm-actions ${SNS_TOPIC} \\\n--cli-input-json file://templates/ho11y-400-alarm.json\n\naws cloudwatch describe-alarms --region ${CAP_CLUSTER_REGION} \\\n--alarm-names \"400 errors from ho11y app\"\n</code></pre>"},{"location":"capture-errors-k8s-slack/#generate-traffic-to-ho11y-application","title":"Generate Traffic to ho11y application","text":"<p>Generate traffic to ho11y application using below mentioned command which in-turn generates metrics. Run this script in a separate terminal :</p> <pre><code>frontend_pod=`kubectl get pod -n ho11y --no-headers -l app=frontend -o jsonpath='{.items[*].metadata.name}'`\nloop_counter=0\nwhile [ $loop_counter -le 5000 ] ;\ndo\n    echo $loop_counter;\n    kubectl exec -n ho11y -it $frontend_pod -- curl frontend.ho11y.svc.cluster.local;\n    loop_counter=$[$loop_counter+1];\ndone\n</code></pre> <p>Let this run for 10 minutes and check CloudWatch alarm status. If there is a breach in threshold, you will get notification sent to Slack channel as shown below. </p> <p></p> <p></p>"},{"location":"capture-errors-k8s-slack/#cleanup","title":"Cleanup","text":"<p>Run <code>cleanup.sh</code> script to clean up all resources deployed as part of this post.</p> <pre><code>sh ./cleanup.sh\n</code></pre>"},{"location":"capture-errors-k8s-slack/#conclusion","title":"Conclusion","text":"<p>In this blog we demonstrated a solution to capture errors from your Kubernetes applications to validate your understanding of operational success of your Kubernetes applications and how it changes over time. We used Amazon EKS Blueprints to create your Amazon EKS clusters with Day 2 operational tooling such as AWS Distro for Open Telemetry (ADOT) and ADOT CloudWatch Collector. We then deployed a AWS Serverless Application Model (SAM) template to generate CloudWatch alarms and invoke an Amazon Lambda function to send real time alerts to Slack for errors from your Kubernetes application. We would recommend you to try this solution and for further learning please check our AWS Observability Best practices guide. Additionally, you can get hands-on experience with the AWS services using the One Observability Workshop. </p> <p></p>"},{"location":"capture-errors-k8s-slack/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"capture-errors-k8s-slack/#re-alvarez-parmar","title":"Re Alvarez Parmar","text":"<p>In his role as Containers Specialist Solutions Architect at Amazon Web Services. Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent over 15 years as Enterprise and Software Architect. He is based out of Seattle. You can connect with him on LinkedIn linkedin.com/in/realvarez/</p> <p></p>"},{"location":"capture-errors-k8s-slack/#prakash-srinivasan","title":"Prakash Srinivasan","text":"<p>Prakash is a Solutions Architect with Amazon Web Services. He is a passionate builder and helps customers to modernize their applications and accelerate their Cloud journey to get the best out of Cloud for their business. In his spare time, he enjoys watching movies and spend more time with family. He is based out of Denver, Colorado and you can connect with him on Linkedin at linkedin.com/in/prakash-s</p>"},{"location":"grafana-operator-AMG/","title":"Using Open Source Grafana Operator on your Kubernetes cluster to manage Amazon Managed Grafana","text":""},{"location":"grafana-operator-AMG/#introduction","title":"Introduction","text":"<p>In this modern era, many customers running Kubernetes are moving towards the paradigm of leveraging Kubernetes as a control plane and rarely as a data plane. Their focus is shifted towards workload gravity and reliance on Kubernetes-native controllers to deploy and manage the lifecycle of external resources such as Cloud resources. We have seen customers installing Crossplane and AWS Controllers for Kubernetes (ACK)to create, deploy and manage AWS services. Many customers these days opt to offload the Prometheus and Grafana implementations to managed services and in case of AWS these services are Amazon Managed Service for Prometheus (AMP) and Amazon Managed Grafana (AMG) for monitoring their workloads. Fundamentally what they need is one single API - the Kubernetes API, to control heterogeneous deployments.</p> <p>Though customers look for ways to offload and manage their Grafana implementations to managed services such as AMG, there is no mechanism to achieve this. The grafana-operatoris a Kubernetes operator built to help you manage your Grafana instances inside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. Though Grafana Operator makes it possible to manage Grafana instances using code in a Kubernetes native way, there was no mechanism to intergrate Grafana services deployed outside of the cluster, such as AMG. </p> <p>To address this gap, AWS team collaborated with the grafana-operatorteam and submitted a design proposalto support the integration of external Grafana instances. With this mechanism it will be possible to add external Prometheus compatible data sources (e.g., Amazon Managed Service for Prometheus) and create Grafana dashboards in external grafana instances (e.g., Amazon Managed Grafana) from your Kubernetes cluster. This enables us to use our Kubernetes cluster as a control plane to create Grafana implementations to Amazon Managed Grafana. Upon colloboration and consistently working through deployment stages with the Grafana Operator team (Kubernetes community), this feature is now fully supported in release 5 of Grafana Operator. This ultimately enables us to use GitOps mechanisms using CNCF projects such as Flux and Crossplane to create and manage the lifecyle of resources in AMG.</p> <p>In this blog, we will be demonstrating on how to use Grafana Operator from your Kubernetes cluster to add an AMP data source and create dashboards in AMG in a Kubernetes native way.</p>"},{"location":"grafana-operator-AMG/#solution-architecture","title":"Solution Architecture","text":"<p>The architecture diagram shows the demonstration of Kubernetes cluster as a control plane with using Grafana Operator to setup an identity with AMG, adding AMP as a datasource and creating dashboards on AMG from Amazon EKS cluster in a Kubernetes native way.</p>"},{"location":"grafana-operator-AMG/#solution-walkthrough","title":"Solution Walkthrough","text":""},{"location":"grafana-operator-AMG/#prerequisites","title":"Prerequisites","text":"<p>You will need the following to complete the steps in this post:</p> <ul> <li>AWS CLI version 2</li> <li>AWS CDK version 2.66.0 or later</li> <li>Node version 18.12.1 or later</li> <li>NPM version 8.19.2 or later</li> <li>Kubectl</li> <li>Git</li> <li>jq</li> <li>Helm</li> <li>An existing Amazon Managed Grafana Workspace</li> </ul> <p>Let\u2019s start by setting a few environment variables:</p> <pre><code>export GO_ACCOUNT_ID=$(aws sts get-caller-identity --query 'Account' --output text)\nexport GO_AWS_REGION=us-west-2\nexport GO_CLUSTER_NAME=\"grafana-operator-cluster\"\nexport GO_AMP_WORKSPACE_NAME=\"demo-amp-Workspace\"\n</code></pre> <p>Clone the sample repository which contains the code for our solution :</p> <pre><code>git clone https://github.com/aws-samples/containers-blog-maelstrom.git\ncd ./containers-blog-maelstrom/grafana-operator-AMG\n</code></pre>"},{"location":"grafana-operator-AMG/#bootstrap-the-environment","title":"Bootstrap the Environment","text":"<p>In this solution we will be using Amazon EKS CDK Blueprints to provision our Amazon EKS cluster. The first step to any CDK deployment is bootstrapping the environment. <code>cdk bootstrap</code> is a tool in the AWS CDK command-line interface (AWS CLI) responsible for preparing the environment (i.e., a combination of AWS account and AWS Region) with resources required by CDK to perform deployments into that environment. If you already use CDK in a region, you don\u2019t need to repeat the bootstrapping process. </p> <p>Lets run the below commands to bootstrap your environment and install all node dependencies required for deploying the solution:</p> <pre><code>npm install\n`cdk bootstrap aws``:``//$GO_ACCOUNT_ID/$GO_AWS_REGION`\n</code></pre> <p>Next, lets try to grab the workspace if of any existing Amazon Managed Grafana workspace :</p> <pre><code>aws grafana list-workspaces\n\n{\n    \"workspaces\": [\n        {\n            \"authentication\": {\n                \"providers\": [\n                    \"AWS_SSO\"\n                ]\n            },\n            \"created\": \"2022-02-20T23:26:19.829000+00:00\",\n            \"description\": \"Demo AMG Workspace\",\n            \"endpoint\": \"g-XXXXXXXXXX.grafana-workspace.us-west-2.amazonaws.com\",\n            \"grafanaVersion\": \"8.4\",\n            \"id\": \"g-XXXXXXXX\",\n            \"modified\": \"2022-02-20T23:26:19.829000+00:00\",\n            \"name\": \"EKSA-Demo-AMG-Workspace\",\n            \"notificationDestinations\": [\n                \"SNS\"\n            ],\n            \"status\": \"ACTIVE\",\n            \"tags\": {}\n        }\n}\n\n# populate the values from `id` and `endpoint` to below environment variable\n\nexport GO_AMG_WORKSPACE_ID=\"&lt;&lt;Your-WORKSPACE-ID&gt;&gt;\"\nexport GO_AMG_WORKSPACE_URL=\"https://&lt;&lt;YOUR-WORKSPACE-URL&gt;&gt;\n</code></pre> <p>Next, lets create a grafana api key from your AMG workspace and setup a secret on AWS Secrets Manager which will be used to access from external secrets by our Amazon EKS cluster :</p> <pre><code>export GO_AMG_API_KEY=$(aws grafana create-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  --key-role \"ADMIN\" \\\n  --seconds-to-live 432000 \\\n  --workspace-id $GO_AMG_WORKSPACE_ID \\\n  --query key \\\n  --output text)\n\nexport GO_API_KEY_SECRET_ARN=$(aws secretsmanager create-secret \\\n    --name grafana-api-key \\\n    --description \"API Key of your Grafana Instance\" \\\n    --secret-string $GO_AMG_API_KEY \\\n    --region $GO_AWS_REGION \\\n    --query ARN \\\n    --output text)\n</code></pre> <p>Please navigate to <code>bin/cluster-blueprint.ts</code> in the cloned repo to check on the Amazon EKS CDK Blueprints stack which will deploy EKS Cluster with day 2 operational add-ons required to run our solution. Please see the below <code>bin/cluster-blueprint.ts</code> snippet showing our EKS CDK Blueprints stack :</p> <pre><code>#!/usr/bin/env node\n\nimport 'source-map-support/register';\nimport * as cdk from 'aws-cdk-lib';\nimport * as blueprints from '@aws-quickstart/eks-blueprints';\nimport { GrafanaOperatorSecretAddon } from './grafanaoperatorsecretaddon';\n\nconst app = new cdk.App();\n\nconst account = process.env.GO_ACCOUNT_ID! || process.env.CDK_DEFAULT_ACCOUNT!;\nconst region = process.env.GO_AWS_REGION! || process.env.CDK_DEFAULT_REGION!;\nconst clusterName = process.env.GO_CLUSTER_NAME!;\nconst ampWorkspaceName = process.env.GO_AMP_WORKSPACE_NAME! || 'demo-amp-Workspace';\n\nconst addOns: Array&lt;blueprints.ClusterAddOn&gt; = [\n    new blueprints.addons.AwsLoadBalancerControllerAddOn(),\n    new blueprints.addons.VpcCniAddOn(),\n    new blueprints.addons.CoreDnsAddOn(),\n    new blueprints.addons.KubeProxyAddOn(),\n    new blueprints.addons.CertManagerAddOn(),\n    new blueprints.addons.ExternalsSecretsAddOn(),\n    new blueprints.addons.PrometheusNodeExporterAddOn(),\n    new blueprints.addons.KubeStateMetricsAddOn(),\n    new blueprints.addons.AdotCollectorAddOn(),\n    new blueprints.addons.AmpAddOn({\n        workspaceName: ampWorkspaceName,\n    }),\n    new GrafanaOperatorSecretAddon(),\n\n];\n\nconst stack = blueprints.EksBlueprint.builder()\n    .account(account)\n    .region(region)\n    .addOns(...addOns)\n    .build(app, clusterName);\n</code></pre> <p>Next, run the <code>cdk list</code> command which lists name of stack that will be created.</p> <pre><code>npm install\ncdk list\n</code></pre> <p>If you are interested in knowing list of resources that will be created by this stack, you can view them using <code>cdk diff</code> command. </p>"},{"location":"grafana-operator-AMG/#create-the-clusters-and-deploy-the-addons","title":"Create the clusters and deploy the addons","text":"<p>Run below command to deploy the Amazon EKS cluster with day 2 operational add-ons required to run our solution. </p> <pre><code>cdk deploy \"*\" --require-approval never\n</code></pre> <p>Deployment will take approximately 20-30 minutes to complete. Upon completion, you will have a fully functioning EKS cluster deployed in your account.</p> <p>This blueprint will deploy the following:</p> <ul> <li>Amazon Virtual Private Cloud (Amazon VPC) with both Public and Private subnets</li> <li>An Amazon EKS cluster in the region and account you specify</li> <li>Amazon VPC CNI Add-on to your EKS cluster to support native VPC networking</li> <li>External Secrets Addon to integrate with AWS Secrets Manager to pull Amazon Managed Grafana api key.</li> <li>CoreDNS Addon is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. </li> <li>CertManager Addon to install and manage the AWS Distro for OpenTelemetry (ADOT) Operator. </li> <li>KubeStateMetrics Addon is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.</li> <li>PrometheusNodeExporter Addon enables you to measure various machine resources such as memory, disk and CPU utilization.</li> <li>Adot Addon to install and manage the AWS Distro for OpenTelemetry (ADOT) Operator. </li> <li>Amp Adot Addon deploys an AWS Distro for OpenTelemetry (ADOT) Collector for Amazon Managed Service for Prometheus (AMP) which receives transactional metrics from the application and Prometheus metrics scraped from pods on the cluster and remote writes the metrics to AMP remote write endpoint. This addon creates an AMP workspace with name <code>demo-amp-wokspace</code>. </li> <li>Creates a ClusterSecretStore which can be used by all external secrets from all namespaces</li> <li>Creates an ExternalSecret which can be used to fetch, transform and inject secret data AMG API Key</li> </ul> <p>Once the deployment is complete, you will see the below output in your terminal:</p> <pre><code>Outputs:\ngrafana-operator-cluster.grafanaoperatorclusterClusterName861294B8 = grafana-operator-cluster\ngrafana-operator-cluster.grafanaoperatorclusterConfigCommandDD46DC5D = aws eks update-kubeconfig --name grafana-operator-cluster --region us-west-2 --role-arn arn:aws:iam::940019131157:role/grafana-operator-cluster-grafanaoperatorclusterMas-3E4ITHRP3FRI\ngrafana-operator-cluster.grafanaoperatorclusterGetTokenCommand6228506B = aws eks get-token --cluster-name grafana-operator-cluster --region us-west-2 --role-arn arn:aws:iam::940019131157:role/grafana-operator-cluster-grafanaoperatorclusterMas-3E4ITHRP3FRI\nStack ARN:\narn:aws:cloudformation:us-west-2:940019131157:stack/grafana-operator-cluster/ee4f6ac0-c460-11ed-9e3a-022cbdd1a7dd\n</code></pre> <p>To update your Kubernetes config for you new cluster, copy and run the <code>aws eks update-kubeconfig...</code> command (the second command in the list above) in your terminal.</p> <pre><code>aws eks update-kubeconfig \\\n    --name $GO_CLUSTER_NAME \\\n    --region $GO_AWS_REGION \\\n    --role-arn arn:aws:iam::$GO_ACCOUNT_ID:role/grafana-operator-cluster-grafanaoperatorclusterMas-3E4ITHRP3FRI\n</code></pre> <p>Validate the access to your EKS cluster using below <code>kubectl</code> listing the secret created to access AMG workspace:</p> <pre><code>\u276f k get secret                                                                                                                                                                     \u2500\u256f\nNAME                        TYPE     DATA   AGE\ngrafana-admin-credentials   Opaque   1      2m59s\n</code></pre> <p>Lets also grab the endpoint URL of your created AMP workspace using the below commands :</p> <pre><code>export GO_AMP_WORKSPACE_ID=$(aws amp list-workspaces \\\n--alias 'demo-amp-Workspace' \\\n--query 'workspaces[].workspaceId' \\\n--output text)\n\nexport GO_AMP_ENDPOINT_URL=\"https://aps-workspaces.${GO_AWS_REGION}.amazonaws.com/workspaces/${GO_AMP_WORKSPACE_ID}\"\n</code></pre>"},{"location":"grafana-operator-AMG/#installing-grafana-operator","title":"Installing Grafana Operator","text":"<p>Next, lets install Grafana Operator on Amazon EKS to manage external Grafana instances such as Amazon Managed Grafana. The Grafana-operator will be used to create an AMP data source and dashboards on AMG using Kubernetes Custom Resource Definitions in a Kubernetes native way. Please use the below command to perform an Helm installation of Grafana Operator :</p> <pre><code>`&gt;`` helm upgrade ``-``i grafana``-``operator`` oci``:``//ghcr.io/grafana-operator/helm-charts/grafana-operator --version v5.0.0-rc0\n\nRelease \"grafana-operator\" does not exist. Installing it now.\nPulled: ghcr.io/grafana-operator/helm-charts/grafana-operator:v5.0.0-rc0\nDigest: sha256:12c28961333134da98890488c6ecfb2012bdf1900d76cf02b57b470cc7e70957\nNAME: grafana-operator\nLAST DEPLOYED: Fri Mar 17 02:06:56 2023\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n`\n</code></pre> <p>Next, run the below command to status of Grafana Operator Helm installation :</p> <pre><code>&gt; kubectl get all\n\nNAME                                                READY   STATUS    RESTARTS   AGE\npod/grafana-operator-5cd6677459-56b4n               2/2     Running   0          14s\npod/otel-collector-amp-collector-59c7c9b4fb-c6qwz   1/1     Running   0          28m\n\nNAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nservice/grafana-operator-metrics-service          ClusterIP   172.20.47.157   &lt;none&gt;        8443/TCP   15s\nservice/kubernetes                                ClusterIP   172.20.0.1      &lt;none&gt;        443/TCP    39m\nservice/otel-collector-amp-collector-monitoring   ClusterIP   172.20.30.253   &lt;none&gt;        8888/TCP   28m\n\nNAME                                           READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grafana-operator               1/1     1            1           15s\ndeployment.apps/otel-collector-amp-collector   1/1     1            1           28m\n\nNAME                                                      DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grafana-operator-5cd6677459               1         1         1       16s\nreplicaset.apps/otel-collector-amp-collector-59c7c9b4fb   1         1         1       28m\n</code></pre>"},{"location":"grafana-operator-AMG/#creating-amazon-managed-grafana-datasources-and-dashboards-using-grafana-operator","title":"Creating Amazon Managed Grafana Datasources and Dashboards using Grafana Operator:","text":"<p>Now, lets get to the fun part to creating an identity to Amazon Managed Grafana using the Grafana API Key from your Amazon EKS cluster. We will be using <code>grafanas.grafana.integreatly.org</code> CRD for this purpose as shown below:</p> <pre><code>`cat ``&gt;`` amg_grafana``-``identity``.``json ``&lt;&lt;`` EOF`\napiVersion: grafana.integreatly.org/v1beta1\nkind: Grafana\nmetadata:\n  name: external-grafana\n  labels:\n    dashboards: \"external-grafana\"\nspec:\n  external:\n    url: $GO_AMG_WORKSPACE_URL\n    apiKey:\n      name: grafana-admin-credentials\n      key: GF_SECURITY_ADMIN_APIKEY\nEOF\nkubectl apply -f amg_grafana-identity.yaml\n</code></pre> <p>Lets check if an identity to Amazon Managed Grafana is created fine using below command :</p> <pre><code>\u276f kubectl get grafanas.grafana.integreatly.org  \n                                                                                                                                   \u2500\u256f\nNAME               AGE\nexternal-grafana   32s\n</code></pre> <p>Next, lets create Amazon Managed Service for Prometheus as a datasource to AMG from your Amazon EKS cluster. We will be using <code>grafanadatasources.grafana.integreatly.org</code> CRD for this purpose as shown below:</p> <pre><code>cat &gt; amg_grafana-amp-datasource.json &lt;&lt; EOF\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDatasource\nmetadata:\n  name: grafanadatasource-sample-amp\nspec:\n  instanceSelector:\n    matchLabels:\n      dashboards: \"external-grafana\"\n  datasource:\n    name: grafana-operator-amp-datasource\n    type: prometheus\n    access: proxy\n    url: $GO_AMP_ENDPOINT_URL\n    isDefault: true\n    jsonData:\n      'tlsSkipVerify': false\n      'timeInterval': \"5s\"\n      'sigV4Auth': true\n      'sigV4AuthType': \"ec2_iam_role\"\n      'sigV4Region': $GO_AWS_REGION\n    editable: true \nEOF\nkubectl apply -f amg_grafana-amp-datasource.yaml\n</code></pre> <p>Lets check if Amazon Managed Service for Prometheus is created as a datasource to AMG from your Amazon EKS cluster using below commands :</p> <pre><code>\u276f kubectl get grafanadatasources.grafana.integreatly.org                                                                                                                                 \u2500\u256f\n\nNAME                           AGE\ngrafanadatasource-sample-amp   2m26s\n</code></pre> <pre><code>\u276f kubectl logs grafana-operator-5cd6677459-56b4n --tail=5\n                                                                                                                                \u2500\u256f\nDefaulted container \"grafana-operator\" out of: grafana-operator, kube-rbac-proxy\n1.6790188337222888e+09    INFO    dashboard sync complete\n1.6790188337222807e+09    INFO    datasources sync complete\n1.6790188337222986e+09    INFO    folder sync complete\n1.679019065074347e+09    INFO    found matching Grafana instances for datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"0e565db4-b332-4dbd-9181-429c9eefd0ab\", \"count\": 1}\n1.6790190652982466e+09    INFO    found matching Grafana instances for datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"eb24f960-d026-4181-9b96-b7609146c524\", \"count\": 1}\n</code></pre> <p>Please check on grafana-operator-manifests if your looking for samples to Amazon CloudWatch and AWS X-Ray as a datasource to AMG from your Amazon EKS Cluster.</p> <p>Next lets navigate to AMG console and click on <code>Configuration</code> \u2192 <code>Data Sources</code> and click on the data source <code>grafana-operator-amp-datasource</code> as shown below :</p> <p> </p> <p>Next, lets click on <code>Save and Test</code> as shown to make sure the data source is working fine.</p> <p></p> <p>Finally lets create a Grafana Dashboard on AMG from your Amazon EKS Cluster. We will be using <code>grafanadashboards.grafana.integreatly.org</code> CRD for this purpose as shown below :</p> <pre><code>cat &gt; amg_grafana-dashboard.json &lt;&lt; EOF\napiVersion: grafana.integreatly.org/v1beta1\nkind: GrafanaDashboard\nmetadata:\n  name: external-grafanadashboard-url\nspec:\n  instanceSelector:\n    matchLabels:\n      dashboards: \"external-grafana\"\n  url: \"https://raw.githubusercontent.com/aws-samples/containers-blog-maelstrom/main/grafana-operator-AMG/dashboards/nodeexporter-nodes.json\"\nEOF\nkubectl apply -f amg_grafana-dashboard.yaml\n</code></pre> <p>Lets now check if Grafana Dashboard on AMG is created from your Amazon EKS Cluster using below command :</p> <pre><code>\u276f kubectl get grafanadashboards.grafana.integreatly.org    \n                                                                                                                              \u2500\u256f\nNAME                            AGE\nexternal-grafanadashboard-url   36s\n</code></pre> <pre><code>\u276f kubectl logs grafana-operator-5cd6677459-56b4n --tail=5                             \n                                                                                                   \u2500\u256f\nDefaulted container \"grafana-operator\" out of: grafana-operator, kube-rbac-proxy\n1.679019065074347e+09    INFO    found matching Grafana instances for datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"0e565db4-b332-4dbd-9181-429c9eefd0ab\", \"count\": 1}\n1.6790190652982466e+09    INFO    found matching Grafana instances for datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"eb24f960-d026-4181-9b96-b7609146c524\", \"count\": 1}\n1.6790193653035195e+09    INFO    found matching Grafana instances for datasource    {\"controller\": \"grafanadatasource\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDatasource\", \"GrafanaDatasource\": {\"name\":\"grafanadatasource-sample-amp\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"grafanadatasource-sample-amp\", \"reconcileID\": \"f427a451-3400-4649-b662-8dba25866641\", \"count\": 1}\n1.6790195211297286e+09    INFO    found matching Grafana instances for dashboard    {\"controller\": \"grafanadashboard\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDashboard\", \"GrafanaDashboard\": {\"name\":\"external-grafanadashboard-url\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"external-grafanadashboard-url\", \"reconcileID\": \"f9dcb08c-58ea-49a4-a3ad-7a9ced94d093\", \"count\": 1}\n1.679019521481265e+09    INFO    found matching Grafana instances for dashboard    {\"controller\": \"grafanadashboard\", \"controllerGroup\": \"grafana.integreatly.org\", \"controllerKind\": \"GrafanaDashboard\", \"GrafanaDashboard\": {\"name\":\"external-grafanadashboard-url\",\"namespace\":\"default\"}, \"namespace\": \"default\", \"name\": \"external-grafanadashboard-url\", \"reconcileID\": \"0ff702f2-4332-4516-aef6-3da20fa80cc6\", \"count\": 1}\n</code></pre> <p>Finally lets navigate to AMG console, click on <code>Search Dashboards</code> and you will be able to see a Dashboard by name <code>Grafana Operator - Node Exporter/Nodes</code> and click on the same will show you the Grafana Dashboard created out of the box having all the metrics from Prometheus Node Exporter installed on your Amazon EKS Cluster.</p> <p></p>"},{"location":"grafana-operator-AMG/#gitops-approach-with-grafana-operator","title":"GitOps Approach with Grafana Operator","text":"<p>GitOps is a way of managing application and infrastructure deployment so that the whole system is described declaratively in a Git repository. It is an operational model that offers you the ability to manage the state of multiple Kubernetes clusters leveraging the best practices of version control, immutable artifacts, and automation. Flux is a declarative, GitOps-based continuous delivery tool that can be integrated into any CI/CD pipeline. It gives users the flexibility of choosing their Git provider (GitHub, GitLab, BitBucket). Crossplane is an open-source Kubernetes add-on that enables platform teams to assemble cloud infrastructure resources, without having to write any code. Now, with grafana-operatorsupporting the management of external Grafana instances such as Amazon Managed Grafana, operations personas can use GitOps mechanisms using CNCF projects such as Flux and Crossplane to create and manage the lifecyle of resources in Amazon Managed Grafana.</p>"},{"location":"grafana-operator-AMG/#cleanup","title":"Cleanup","text":"<p>You continue to incur cost until deleting the infrastructure that you created for this post. Use the commands below to delete resources created during this post:</p> <pre><code>kubectl delete -f amg_grafana-dashboard.yaml\nkubectl delete -f amg_grafana-amp-datasource.yaml\nkubectl delete -f amg_grafana-identity.yaml\nhelm delete grafana-operator\naws secretsmanager delete-secret \\\n    --secret-id $GO_API_KEY_SECRET_ARN \\\n    --recovery-window-in-days 7\naws grafana delete-workspace-api-key \\\n  --key-name \"grafana-operator-key\" \\\n  `--``workspace``-``id $GO_AMG_WORKSPACE_ID`\n`cdk destroy ``\"*\"`\n</code></pre> <p>CDK will ask you <code>Are you sure you want to delete: grafana-operator-cluster (y/n)?</code> and enter <code>y</code> to delete.</p>"},{"location":"grafana-operator-AMG/#conclusion","title":"Conclusion","text":"<p>In this blog, we started talking about how organizations are leverage kubernetes as a control plane to create and manage Grafana implementations to managed services such as Amazon Managed Grafana. Further, we demonstrated on how to use Grafana Operator from your Kubernetes cluster to data sources such as Amazon Managed Service for Prometheus and create Grafana dashboards to external grafana instances such as Amazon Managed Grafana in a Kubernetes native way. We would high encourage to try this solution and also leverage GitOps mechanisms with Grafana Operator using CNCF projects such as Flux and Crossplane to create and manage the lifecyle of resources in Amazon Managed Grafana.</p> <p>For more information, see the following references:</p> <ul> <li>Grafana Operator</li> <li>Kubernetes as a platform vs. Kubernetes as an API</li> <li>Amazon EKS Blueprints</li> <li>Amazon EKS Blueprints Patterns</li> <li>GitOps model for provisioning and bootstrapping Amazon EKS clusters using Crossplane and Flux</li> </ul>"},{"location":"grafana-operator-AMG/#authors","title":"Authors","text":""},{"location":"grafana-operator-AMG/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"grafana-operator-AMG/#mikhail-shapirov","title":"Mikhail Shapirov","text":"<p>Mikhail is a Principal Partner Solutions Architect at AWS, focusing on container services. Mikhail helps partners and customers drive their products and services on AWS, integrating with Amazon ECS, EKS and AppMesh. He is also a software engineer.</p> <p></p>"},{"location":"grafana-operator-AMG/#edvin-norling","title":"Edvin Norling","text":"<p>Edvin Norling is a Platform Engineer at Kognic and creates a golden path for hes developers using Kubernetes and surrounding projects. Edvin is the maintainer of the grafana-opertor which is a Kubernetes operator to manage Grafana instances. He is based out of Gothenburg Sweden and you can reach him on Kubernetes slack @Edvin N</p>"},{"location":"prefetch-data-to-eks-nodes/","title":"Event Driven process to prefetch data to EKS Nodes using SSM Automation","text":""},{"location":"prefetch-data-to-eks-nodes/#introduction","title":"Introduction","text":"<p>To-Do</p> <p>Benefits of Event-Driven Data Prefetching Event-driven data prefetching provides several benefits, including:</p> <p>Improved performance: By fetching data in anticipation of future requests, you can reduce the latency and improve the overall user experience.</p> <p>Reduced server load: By fetching data ahead of time, you can reduce the load on your servers, allowing them to handle more requests.</p> <p>Increased reliability: By automating the process of fetching data, you can reduce the risk of errors and improve the reliability of your system.</p> <p>In this blog, we will demonstrate the usage of AWS Systems Manager SSM Automation and State Manager to prefetch container images to your existing and newer worker nodes of your Amazon EKS Cluster.</p>"},{"location":"prefetch-data-to-eks-nodes/#solution-overview","title":"Solution Overview","text":"<p>Below is the overall architecture for setting up Event Driven process to prefetch data to EKS Nodes using SSM Automation</p> <p></p> <p>The process for implementing this solution is as follows:</p> <ul> <li>The first step is to identify the image repository to fetch the container image. The container image repository could be Amazon Elastic Container Registry (Amazon ECR), DockerHub or others. For this demonstration we are using Amazon ECR as the image source.</li> <li>Next, when a container image gets pushed to Amazon ECR, an event based rule is triggered  by Amazon EventBridge to trigger an AWS SSM automation to prefetch container images from Amazon ECR to your existing Amazon EKS worker nodes.</li> <li>Whenever a newer worker node gets added to your Amazon EKS cluster, based on the tags on the worker node, Systems Manager State Manager Association on tags acts on to prefetch container images to newly created worker nodes.</li> </ul>"},{"location":"prefetch-data-to-eks-nodes/#solution-walkthrough","title":"Solution Walkthrough","text":""},{"location":"prefetch-data-to-eks-nodes/#prerequisites","title":"Prerequisites","text":"<p>To run this solution, you must have the following prerequisites:</p> <ul> <li>AWS CLI version 2.10 or higher to interact with AWS services</li> <li>eksctl for creating and managing your Amazon EKS cluster</li> <li>kubectl for running kubectl commands on your Amazon EKS cluster</li> <li>envsubst for environment variables substitution (envsubst is included in gettext package)</li> <li>jq for command-line JSON processing</li> </ul>"},{"location":"prefetch-data-to-eks-nodes/#source-code","title":"Source Code","text":"<p>Checkout the source code, the source code for this blog is available in AWS-Samples on [GitHub] (https://github.com/aws-samples/containers-blog-maelstrom/tree/main/prefetch-data-to-EKSnodes)</p> <pre><code>mkdir aws-prefetch-data-to-EKSnodes &amp;&amp; cd aws-prefetch-data-to-EKSnodes\ngit clone https://github.com/aws-samples/containers-blog-maelstrom/tree/main/prefetch-data-to-EKSnodes .\n</code></pre>"},{"location":"prefetch-data-to-eks-nodes/#implementation-steps","title":"Implementation Steps","text":"<ol> <li> <p>Let\u2019s start by setting a few environment variables.</p> <pre><code>export EDP_AWS_REGION=us-east-1\nexport EDP_AWS_ACCOUNT=$(aws sts get-caller-identity --query 'Account' --output text)\nexport EDP_NAME=prefetching-data-automation\n</code></pre> </li> <li> <p>Create Amazon Elastic Container Registry repository.</p> <pre><code>aws ecr create-repository \\\n--cli-input-json file://repo.json  \\\n--repository-name ${EDP_NAME} </code></pre> </li> <li> <p>Create an Amazon EKS Cluster using the below commands. Using envsubst utility we will be replacing the variables in the Yaml Config file and using the eksctl CLI tool will deploy the cluster.</p> <pre><code>envsubst &lt; cluster-config.yaml | eksctl create cluster -f -\n</code></pre> </li> <li> <p>Build a large docker image size of approximately 1 GB to test this solution by running below shell script.</p> <pre><code>./build-docker-image.sh\n</code></pre> </li> <li> <p>Create prefetching-data-automation-role with trust policy events-trust-policy.json which will be assumed by Amazon EventBridge service.</p> <pre><code>aws iam create-role \\\n--role-name $EDP_NAME-role \\\n--assume-role-policy-document file://events-trust-policy.json\n</code></pre> </li> <li> <p>Run the below command to replace variables in events-policy.json policy file by using envsubst utility and attach the policy to the above created 'prefetching-data-automation-role' role.</p> <pre><code>aws iam put-role-policy \\\n--role-name ${EDP_NAME}-role \\\n--policy-name ${EDP_NAME}-policy \\\n--policy-document \"$(envsubst &lt; events-policy.json)\"\n</code></pre> </li> <li> <p>Create Amazon EventBridge Rule to trigger SSM Run Command on successful ECR Image push, using envsubst we will be replacing the variables in the events-rule.json file.</p> <pre><code>envsubst &lt; events-rule.json &gt; events-rule-updated.json \\\n&amp;&amp; aws events put-rule --cli-input-json file://events-rule-updated.json \\\n&amp;&amp; rm events-rule-updated.json\n</code></pre> </li> <li> <p>Attach the Target as AWS Systems Manager Run Command to AWS EventBridge Rule created above, using envsubst we will be replacing the variables in the events-target.json file.</p> <pre><code>envsubst '$EDP_AWS_REGION $EDP_AWS_ACCOUNT $EDP_NAME' &lt; events-target.json &gt; events-target-updated.json \\\n&amp;&amp; aws events put-targets --rule $EDP_NAME --cli-input-json file://events-target-updated.json \\\n&amp;&amp; rm events-target-updated.json   </code></pre> </li> <li> <p>Create AWS Systems Manager State Manager Association for new worker nodes to prefetch container images, using envsubst we will be replacing the variables in the statemanager-association.json file.</p> <pre><code>envsubst '$EDP_AWS_REGION $EDP_AWS_ACCOUNT $EDP_NAME' &lt; statemanager-association.json &gt; statemanager-association-updated.json \\\n&amp;&amp; aws ssm create-association --cli-input-json file://statemanager-association-updated.json \\\n&amp;&amp; rm statemanager-association-updated.json   </code></pre> <p>Note: Status might show failed for the AWS SSM State Manager association as there is no image present in ECR yet.</p> </li> </ol>"},{"location":"prefetch-data-to-eks-nodes/#validation","title":"Validation","text":"<p>Now the setup is complete, let\u2019s run some validations on the setup for Event Driven process to prefetch data to EKS Nodes.</p>"},{"location":"prefetch-data-to-eks-nodes/#first-test","title":"First test","text":"<p>Verify if the container images are getting fetched to existing worker nodes automatically upon a container image push. </p> <ol> <li> <p>Run the following command to get authenticated with ECR repository.</p> <pre><code>aws ecr get-login-password --region $EDP_AWS_REGION | \\\ndocker login --username AWS --password-stdin $EDP_AWS_ACCOUNT.dkr.ecr.$EDP_AWS_REGION.amazonaws.com\n</code></pre> </li> <li> <p>Push the container image created in step 4 of Implementations step to Amazon ECR</p> <pre><code>docker push $EDP_AWS_ACCOUNT.dkr.ecr.$EDP_AWS_REGION.amazonaws.com/$EDP_NAME\n</code></pre> </li> <li> <p>Check if the event rule we created on the Amazon EventBridge has been triggered. In your Amazon EventBridge console, Navigate to TriggeredRules under Monitoring tab. If there are no FailedInvocations     datapoints, then EventBridge has delivered the event to the target successfully which in this case is AWS Systems Manager Run Command. </p> <p></p> <p>Note: It might take 3 to 5 mins for the data points to be published in the Monitoring graphs.</p> </li> <li> <p>Verify if AWS Systems Manager Run Command is triggered by Amazon EventBridge. Run the below command to see the invocations. Look for DocumentName which should be AWS-RunShellScript, RequestedDateTime to identify     corresponding run, and then status to make sure if the Run Command executed Successfully or not.</p> <pre><code>aws ssm list-command-invocations \\\n--details \\\n--filter \"[{\\\"key\\\": \\\"DocumentName\\\", \\\"value\\\": \\\"arn:aws:ssm:us-east-1::document/AWS-RunShellScript\\\"}]\"\n</code></pre> <p>Output</p> <pre><code>{\n\"CommandInvocations\": [\n{\n\"CommandId\": \"eeb9d869-421d-488f-b1ba-ce93a69db2b0\",\n\"InstanceId\": \"i-0e1a4977c389*****\",\n\"InstanceName\": \"ip-192-168-29-214.ec2.internal\",\n\"Comment\": \"\",\n\"DocumentName\": \"arn:aws:ssm:us-east-1::document/AWS-RunShellScript&lt;/span\",\n\"DocumentVersion\": \"$DEFAULT\",\n\"RequestedDateTime\": \"2023-02-17T17:35:48.520000-06:00\",\n\"Status\": \"Success\",\n\"StatusDetails\": \"Success\",\n.......\n.......\n}\n]\n}\n</code></pre> </li> <li> <p>Verify if the Image has been copied in to worker node of your Amazon EKS Cluster using the below command.</p> <pre><code>aws ec2 describe-instances \\\n--filters \"Name=tag:eks:cluster-name,Values=$EDP_NAME\" \"Name=tag:eks:nodegroup-name,Values=nodegroup\" \\\n--query \"Reservations[*].Instances[*].InstanceId\" \\\n--output text | xargs -I {} aws ssm start-session \\\n--target {} \\\n--document-name AWS-StartInteractiveCommand \\\n--parameters \"command=echo \\$(curl -s http://169.254.169.254/latest/meta-data/instance-id) &amp;&amp; sudo docker images\" \\\n--region $EDP_AWS_REGION\n</code></pre> <p>Output</p> <pre><code>Starting session with SessionId: nbbat-0cf87cdf534*****\n........\nREPOSITORY                                                                 TAG                  IMAGE ID       CREATED          SIZE 0266528*****.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation   latest               d50f7ccece64   50 minutes ago   1.23GB\n.......\n</code></pre> </li> </ol>"},{"location":"prefetch-data-to-eks-nodes/#second-test","title":"Second Test","text":"<p>Validate the container image getting copied to new worker node for any newly added Amazon EKS worker node.</p> <ol> <li> <p>Lets create new worker node as part of EKS Cluster using below command.</p> <pre><code>eksctl scale nodegroup \\\n--cluster $EDP_NAME \\\n--name nodegroup \\\n--nodes 2 \\\n--nodes-min 1\\\n--nodes-max 3\n</code></pre> </li> <li> <p>Verify if the AWS System Manager State Manager Association has been triggered and association execution is successful. </p> <pre><code>aws ssm list-associations \\\n--association-filter-list \"key=AssociationName,value=$EDP_NAME\"\n</code></pre> <p>Note: Please wait for for few minutes for new worker node to come up and run above command</p> <p>Output</p> <pre><code>{\n\"Associations\": [\n{\n\"Name\": \"AWS-RunShellScript\",\n\"AssociationId\": \"d9c82d84-0ceb-4f0f-a8d8-35cd67d1a66e\",\n......\n\"AssociationStatusAggregatedCount\": {\n\"Failed\": 1,\n\"Success\": 1\n}\n},\n\"AssociationName\": \"prefetching-data-automation\"\n]\n}\n</code></pre> </li> <li> <p>Verify if the Image has been copied in to worker node of your Amazon EKS Cluster using the below command.</p> <pre><code>aws ec2 describe-instances \\\n--filters \"Name=tag:eks:cluster-name,Values=$EDP_NAME\" \"Name=tag:eks:nodegroup-name,Values=nodegroup\" \\\n--query \"Reservations[*].Instances[*].InstanceId\" \\\n--output text | xargs -I {} aws ssm start-session \\\n--target {} \\\n--document-name AWS-StartInteractiveCommand \\\n--parameters \"command=echo \\$(curl -s http://169.254.169.254/latest/meta-data/instance-id) &amp;&amp; sudo docker images\" \\\n--region $EDP_AWS_REGION\n</code></pre> <p>Output</p> <pre><code>Starting session with SessionId: nbbat-0cf87cdf5347*****\n........\nREPOSITORY                                                                 TAG                  IMAGE ID       CREATED          SIZE\n0266528*****.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation   latest               d50f7ccece64   50 minutes ago   1.23GB\n.......\n</code></pre> </li> </ol>"},{"location":"prefetch-data-to-eks-nodes/#final-test","title":"Final Test","text":"<p>Lets identify the time difference for a Kubernetes pod to get in to running state with a Container Image pulled from Amazon ECR vs Image pulled locally</p>"},{"location":"prefetch-data-to-eks-nodes/#final-test-a","title":"Final Test A","text":"<ol> <li> <p>Delete the locally cached/copied image from one of the worker nodes using the following commands.</p> <p>Grab the Instance ID</p> <pre><code>InstanceID=$(kubectl get nodes -o jsonpath='{.items[*].spec.providerID}' | awk -F/ '{print $NF}')\n</code></pre> </li> <li> <p>SSH in to the Instance.</p> <pre><code>aws ssm start-session \\\n--target $InstanceID \\\n--region $EDP_AWS_REGION\n</code></pre> </li> <li> <p>List the locally cached image that you pushed in one of the above step.</p> <pre><code>sudo su\n</code></pre> <pre><code>IMAGE_ID=$(docker images | awk 'NR==2{print $3}')\n</code></pre> </li> <li> <p>Delete the locally cached image.</p> <pre><code>docker rmi $IMAGE_ID\n</code></pre> <p>Exit out of root and exit out of the SSM session</p> </li> <li> <p>Pull the latest container image and create a Kubernetes Pod.</p> <pre><code>sh pod.sh\n</code></pre> <pre><code>kubectl apply -f pod.yaml\n</code></pre> </li> <li> <p>Run below command to check how long it took for pod to get in to running state.</p> <pre><code>kubectl describe pod $EDP_NAME\n</code></pre> <p>Output</p> <pre><code>nbbathul@88665a1f8bb5 EDP_Working % kubectl describe pod prefetching-data-automation\nName:         prefetching-data-automation\nNamespace:    default\nPriority:     0\nNode:         ip-192-168-19-136.ec2.internal/192.168.19.136\nStart Time:   Thu, 09 Mar 2023 23:03:52 -0600\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/psp: eks.privileged\nStatus:       Running\nIP:           192.168.23.89\nIPs:\nIP:  192.168.23.89\nContainers:\nprefetching-data-automation:  Container ID:  containerd://29579b61aaca8597bade857458e95b669ab7fca142c1e8f733cfec07d15d9d4d\n   Image:         022435809194.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation:latest\n   Image ID:      022435809194.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation@sha256:d7a93473bd682ed53acbaba18405532e6c1026c35b7d04ffc96ad89d2221736c\n   Port:          &lt;none&gt;\n   Host Port:     &lt;none&gt;\n   Command:\n   sleep\n   3600 State:          Running\n   Started:      Thu, 09 Mar 2023 23:04:52 -0600\n   Ready:          True\n   Restart Count:  0\nEnvironment:    &lt;none&gt;\n</code></pre> <p>Notice time difference between Start Time and Started Time</p> </li> <li> <p>Also validate time taken by pod to get in to running state by running below commands.</p> <pre><code>chmod +x get-pod-boot-time.sh\n</code></pre> <pre><code>for pod in $(kubectl get --no-headers=true pods -o name | awk -F \"/\" '{print $2}'); do ./get-pod-boot-time.sh $pod ; done \\\n&gt;&gt; pod-up-time-with-image-from-ecr.txt\n</code></pre> <pre><code>cat pod-up-time-with-image-from-ecr.txt\n</code></pre> <p>Output</p> <pre><code>It took approximately 60 seconds for pod get in to running state\n</code></pre> </li> </ol>"},{"location":"prefetch-data-to-eks-nodes/#final-test-b","title":"Final Test B","text":"<ol> <li> <p>Delete the Kubernetes Pod, create another pod by using sample pod definition file created above and calculate the time pod take to get to running state, since the image is cached locally this time it shouldn\u2019t take      long to start the pod.</p> <pre><code>kubectl delete pod $EDP_NAME\n</code></pre> <pre><code>kubectl apply -f pod.yaml\n</code></pre> </li> <li> <p>Run below command to check how long it took for pod to get in to running state.</p> <pre><code>kubectl describe pod $EDP_NAME\n</code></pre> <p>Output</p> <pre><code>nbbathul@88665a1f8bb5 EDP_Working % kubectl describe pod prefetching-data-automation\nName:         prefetching-data-automation\nNamespace:    default\nPriority:     0\nNode:         ip-192-168-19-136.ec2.internal/192.168.19.136\nStart Time:   Thu, 09 Mar 2023 23:20:05 -0600\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/psp: eks.privileged\nStatus:       Running\nIP:           192.168.10.39\nIPs:\nIP:  192.168.10.39\nContainers:\nprefetching-data-automation:\nContainer ID:  containerd://fc06a2c5f5ee7734b2a9c4fd893acd1aca7c314ba035b6a01fa9954ae48a69fb\nImage:         022435809194.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation:latest\nImage ID:      022435809194.dkr.ecr.us-east-1.amazonaws.com/prefetching-data-automation@sha256:d7a93473bd682ed53acbaba18405532e6c1026c35b7d04ffc96ad89d2221736c\nPort:          &lt;none&gt;\nHost Port:     &lt;none&gt;\nCommand:\n   sleep\n   3600\nState:          Running\n   Started:      Thu, 09 Mar 2023 23:20:06 -0600\nReady:          True\nRestart Count:  0\nEnvironment:    &lt;none&gt;\n</code></pre> <p>Notice time difference between Start Time and Started Time</p> </li> <li> <p>Also validate time take by pod to get in to running state by running below commands.</p> <pre><code>for pod in $(kubectl get --no-headers=true pods -o name | awk -F \"/\" '{print $2}'); do ./get-pod-boot-time.sh $pod ; done \\\n&gt;&gt; pod-up-time-with-image-from-workernode.txt\n</code></pre> <pre><code>cat pod-up-time-with-image-from-workernode.txt\n</code></pre> <p>Output</p> <pre><code>It took 1 second for pod to get in to running state\n</code></pre> </li> </ol> <p>Below table shows  time it took for Pod that has been created with locally cached image is drastically less when compared to Pod that has been created with image that got pulled from ECR repository.</p> Entity Final Test A (Created Pod by pulling image from ECR repo) Final Test B (Created Pod by pulling locally cached Image) Pod Start Time 23:03:52 -0600 23:20:05 -0600 Pod Running Time 23:04:52 -0600 23:20:06 -0600 Total Time Taken 60 Seconds 1 Second"},{"location":"prefetch-data-to-eks-nodes/#cleanup","title":"Cleanup","text":"<pre><code>chmod +x cleanup.sh\n</code></pre> <pre><code>./cleanup.sh\n</code></pre>"},{"location":"prefetch-data-to-eks-nodes/#conclusion","title":"Conclusion","text":"<p>In this blog, we demonstrated the usage of AWS Systems Manager SSM Automation and State Manager to prefetch container images to your existing and newer worker nodes of your Amazon EKS Cluster. We clearly demonstrated a clear differentiation in run times when your container images are prefetched to worker nodes. This solution will be very effective for run machine learning, analytics and other complex containerized workloads having large container images which otherwise needs lot of time to cache locally.</p> <p>For more information, see the following references:</p> <p></p>"},{"location":"prefetch-data-to-eks-nodes/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"prefetch-data-to-eks-nodes/#re-alvarez-parmar","title":"Re Alvarez Parmar","text":"<p>In his role as Containers Specialist Solutions Architect at Amazon Web Services. Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent over 15 years as Enterprise and Software Architect. He is based out of Seattle. You can connect with him on LinkedIn linkedin.com/in/realvarez/</p> <p></p>"},{"location":"prefetch-data-to-eks-nodes/#naveen-kumar-bathula","title":"Naveen Kumar Bathula","text":"<p>Naveen Bathula is a Partners Solutions Architect with Amazon Web Services. Naveen works with Systems Integrator Partners, being their primary contact for technical questions related to AWS services and solutions and providing best practice guidance to operate on AWS Cloud. Prior to joining AWS, he spent over 5 years as DevOPs Engineer. He is based out of Dallas, TX. You can connect with him on Linkedin linkedin.com/in/naveen-bathula</p>"},{"location":"semantic-versioning-app-runner/","title":"Enable continuous deployment based on semantic versioning using AWS App Runner","text":""},{"location":"semantic-versioning-app-runner/#introduction","title":"Introduction","text":"<p>In this modern cloud era, customers automatically build, test, and deploy the new version of their application multiple times a day, and this is a common scenario in the software development life cycle as it allows for faster delivery of features, bug fixes, and other updates to end users. One key aspect of continuous deployment is semantic versioning, a system for assigning version numbers to software releases. Semantic versioning uses a standard format to convey the level of change in a release, allowing developers and users to understand the potential impact of an update. </p> <p>In this blog post, we will show you how to use semantic versioning combined with the CI/CD capabilities App Runner provides to deploy new versions of the application automatically.</p>"},{"location":"semantic-versioning-app-runner/#semantic-versioning","title":"Semantic Versioning","text":"<p>Semantic versioning is a system for assigning version numbers to software releases. It uses a standard format to convey the level of change in a release, allowing developers and users to understand the potential impact of an update. The basic format of a semantic version number is <code>MAJOR.MINOR.PATCH</code>, where each component is a non-negative integer.</p> <p></p> <p>Here are some general rules for semantic versioning: * When a release contains backward-incompatible changes, the MAJOR version is incremented.  * When a release contains backward-compatible changes, the MINOR version is incremented.  * PATCH version is incremented for releases that contain only bug fixes and no other changes. </p> <p>By using semantic versioning, developers can communicate the impact of a release to users, making it easier to understand the risks and benefits of updating to a new version. It also helps organizations to adopt a more predictable and consistent approach to versioning and releasing their software. Semantic versioning is not a replacement for a changelog or release notes. It is a way to convey the impact of a release, but it does not provide any information about the changes made.</p>"},{"location":"semantic-versioning-app-runner/#problem-statement","title":"Problem Statement","text":"<p>AWS App Runner is a fully managed container application service that makes it easy to deploy containerized applications from source code repositories quickly. App Runner provides a fully controlled environment to build, run, and scale containerized applications. It also provides a fully managed CI/CD pipeline to build and deploy new application versions automatically. Customers can leverage App Runner to continuously monitor their ECR repository for new images based on a fixed tag (like <code>LATEST</code>) and automatically deploy the new version of the application to the underlying App Runner service. </p> <p>However, this approach does not allow customers to monitor and deploy the new version of the application based on semantic versioning. Let's say the customer wants App Runner to automatically deploy the new application version based on a match pattern like <code>&gt;= MAJOR1.MINOR2.PATCH3</code>, this is not possible with the current App Runner capabilities. </p>"},{"location":"semantic-versioning-app-runner/#customer-benefits","title":"Customer Benefits","text":"<p>Here are some of the benefits of using the solution outlined in this post:</p> <ul> <li>Customers can use semantic versioning to communicate the impact of a release to users, making it easier to understand the risks and benefits of updating to a new version.</li> <li>Customers can use App Runner to automatically deploy new versions of the application based on semantic versioning.</li> <li>Customers can use unique tags (based on build ID, git commit) for each version of the application, making tracking and managing the application versions easier.</li> <li>With this approach, customers can start following the best practices in versioning and releasing their software. Yet, they can still leverage App Runner to roll out these changes to their end users without worrying about the underlying infrastructure.</li> <li>The solution outlined in this post is scalable and can deploy multiple applications without additional cost.</li> </ul>"},{"location":"semantic-versioning-app-runner/#solution-overview","title":"Solution Overview","text":"<p>In this solution we use the following AWS services: * AWS App Runner - Fully managed container application service that makes it easy to quickly deploy containerized applications from source code repositories. * AWS Lambda - Serverless compute service that allows you to run code without provisioning or managing servers. * AWS ECR - Fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images. * AWS EventBridge - Fully managed event bus that makes it easy to connect applications together using data from your own applications, Software-as-a-Service (SaaS) applications, and AWS services. * AWS S3 - Fully managed object storage service that offers industry-leading scalability, data availability, security, and performance.</p> <p>The following diagram shows the overall architecture of the solution:</p> <p></p> <p>The solution uses Event bridge rules to listen to the ECR <code>PUSH</code> events, which get processed by a Lambda function via an SQS queue. The lambda function uses AWS App Runner APIs to fetch the currently deployed version of the application and compare it with the <code>imageTag</code> that got pushed to the ECR repository. If there is a match (based on semantic versioning), the Lambda function updates the App Runner service to deploy the new version of the application. Customers can provide the match pattern as an input parameter to the Lambda function in the form of a JSON file (sample below) stored in an S3 bucket.</p> <pre><code>[\n{\n\"repository\": \"Hello-World-AppRunner-Repo\",\n\"semVersion\": \"&gt;1.2.3\",\n\"serviceArn\": \"arn:aws:apprunner:us-east-1:123456789123:service/Hello-World-Service/2d0032a93cbb4cbdaef0966607052336\"\n}\n]\n</code></pre> <p>The solution supports NPM style versioning checks, and here are some examples of the match patterns that are supported: * <code>&gt;1.2.3</code> - Matches any version greater than 1.2.3. * <code>1.1.1 || 1.2.3 - 2.0.0</code> - Matches 1.1.1 version or any version between 1.2.3 &amp; 2.0.0 (including). * <code>1.1.*</code> - Matches any version starting with 1.1. * <code>~1.2.1</code> - Matches any version greater than or equal to 1.2.1 but less than 1.3.0. * <code>^1.2.1</code> - Matches any version greater than or equal to 1.2.1 but less than 2.0.0.</p> <p>The following environment variables need to be set in the Lambda function: * <code>QUEUE_NAME</code> - Name of the SQS queue that will receive the ECR push events and trigger the lambda function. * <code>CONFIG_BUCKET</code> - Name of the S3 bucket that contains the JSON file with the match pattern. * <code>CONFIG_FILE</code> - Name of the JSON file that contains the match pattern (sample provided under <code>config</code> folder).</p> <p>The below sequence diagram shows the interaction between different components of the solution, when a new version of the application gets pushed to the ECR repository:</p> <p></p> <p>Retry Logic: If there are multiple ECR push events on the same repository, the Lambda function will first check whether there is an ongoing deployment for the target App Runner service. If a deployment is in progress, the Lambda function will wait for the deployment to complete and retry again after 10 minutes. Lambda will retry a maximum of 3 times before giving up.</p>"},{"location":"semantic-versioning-app-runner/#pre-requisites","title":"Pre-requisites","text":"<p>To implement this solution, you need the following prerequisites:</p> <ul> <li>The AWS Command Line Interface (AWS CLI) installed. The AWS CLI is a unified tool to manage your AWS services.</li> <li>The AWS CDK installed on your local laptop.</li> <li>Git installed and configured on your machine.</li> <li>jq installed on your machine.</li> </ul>"},{"location":"semantic-versioning-app-runner/#deployment","title":"Deployment","text":""},{"location":"semantic-versioning-app-runner/#setup-ecr-repository-and-app-runner-service","title":"Setup ECR repository and App Runner Service","text":"<p>Note: Let us demonstrate the solution by using the hello world sample application from the App Runner documentation.</p> <ol> <li>Checkout the sample application from Github.</li> </ol> <pre><code>mkdir hello-app-runner &amp;&amp; cd hello-app-runner\ngit clone https://github.com/aws-containers/hello-app-runner.git .\n</code></pre> <ol> <li>Create a new ECR repository.</li> </ol> <pre><code>aws ecr create-repository --repository-name hello-world-apprunner-repo\n</code></pre> <p>Output</p> <pre><code>{\n\"repository\": {\n\"repositoryArn\": \"arn:aws:ecr:us-east-1:&lt;&lt;account&gt;&gt;:repository/hello-world-apprunner-repo\",\n\"registryId\": \"&lt;&lt;account&gt;&gt;\",\n\"repositoryName\": \"hello-world-apprunner-repo\",\n\"repositoryUri\": \"&lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo\",\n\"createdAt\": \"2023-01-02T15:20:14-08:00\",\n\"imageTagMutability\": \"MUTABLE\",\n\"imageScanningConfiguration\": {\n\"scanOnPush\": false\n},\n\"encryptionConfiguration\": {\n\"encryptionType\": \"AES256\"\n}\n}\n}\n</code></pre> <ol> <li>Login to the ECR repository.</li> </ol> <pre><code>aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin &lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com\n</code></pre> <ol> <li>Build the docker image, tag and push it to the ECR repository.</li> </ol> <pre><code>docker build -t hello-world-apprunner-repo .\ndocker tag hello-world-apprunner-repo:latest &lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo:1.2.3\ndocker push &lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo:1.2.3\n</code></pre> <ol> <li>Create a App Runner access role and attach ECR access policy to it.</li> </ol> <pre><code>export TP_FILE=$(mktemp)\nexport ROLE_NAME=AppRunnerSemVarAccessRole\ncat &lt;&lt;EOF | tee $TP_FILE\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"build.apprunner.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\nEOF\naws iam create-role --role-name $ROLE_NAME --assume-role-policy-document file://$TP_FILE\nrm $TP_FILE\naws iam attach-role-policy --role-name $ROLE_NAME --policy-arn arn:aws:iam::aws:policy/service-role/AWSAppRunnerServicePolicyForECRAccess\n</code></pre> <ol> <li>Create a new App Runner service using the ECR repository that was created in the previous step.</li> </ol> <pre><code>aws apprunner create-service --cli-input-json file://input.json\n</code></pre> <p>Contents of <code>input.json</code>: <pre><code>{\n\"ServiceName\": \"hello-world-service\",\n\"SourceConfiguration\": {\n\"AuthenticationConfiguration\": {\n\"AccessRoleArn\": \"arn:aws:iam::${AWS_REGION}:role/${ROLE_NAME}\"\n},        \"ImageRepository\": {\n\"ImageIdentifier\": \"${AWS_ACCOUNT_ID}.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo:1.2.3\",\n\"ImageConfiguration\": {\n\"Port\": \"8000\"\n},\n\"ImageRepositoryType\": \"ECR\"\n}\n},\n\"InstanceConfiguration\": {\n\"CPU\": \"1 vCPU\",\n\"Memory\": \"3 GB\"\n}\n}\n</code></pre></p> <p>Output Keep track of the <code>ServiceArn</code> (sample below) returned in the output. This will be used in the next step.</p> <pre><code>arn:aws:apprunner:us-east-1:&lt;&lt;accountId&gt;:service/hello-world-service/&lt;&lt;serviceId&gt;&gt;\n</code></pre> <ol> <li>Once the service is created, you can access the application using the URL that is displayed in the App Runner console. You should see the following output:</li> </ol> <p></p>"},{"location":"semantic-versioning-app-runner/#deploy-the-solution","title":"Deploy the solution","text":"<ol> <li>Checkout the solution from github</li> </ol> <pre><code>git clone https://github.com/aws-samples/containers-blog-maelstrom.git\ncd containers-blog-maelstrom/sem-var-ecr-watcher-app-runner\n</code></pre> <ol> <li>Update the <code>serviceARN</code> attribute inside <code>config.json</code> file under <code>config/</code> folder with the <code>ServiceArn</code> that was returned in the previous step (sample below).</li> </ol> <pre><code>cat &gt; config/config.json&lt;&lt; EOF\n[\n    {\n      \"repository\": \"hello-world-apprunner-repo\",\n      \"semVersion\": \"&gt;1.2.2\",\n      \"serviceArn\": \"arn:aws:apprunner:${AWS_REGION}:${AWS_ACCOUNT_ID}:service/hello-world-service/${SERVICE_ID}\"\n    }\n  ]\nEOF\n</code></pre> <ol> <li>If you\u2019re running AWS CDK for the first time, run the following command to bootstrap the AWS CDK environment (provide your AWS account ID and AWS Region):</li> </ol> <pre><code>cdk bootstrap \\\n--cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\\naws://&lt;AWS Account Id&gt;/&lt;AWS_REGION&gt;\n</code></pre> <p>Note: You only need to bootstrap the AWS CDK one time (skip this step if you have already done this).</p> <ol> <li>Run the following command to deploy the code:</li> </ol> <pre><code>cdk deploy --requires-approval\n</code></pre>"},{"location":"semantic-versioning-app-runner/#testing","title":"Testing","text":"<p>You must publish a new application version to the ECR repository to test the solution. The latest version should match the semver pattern (<code>&gt;1.2.3</code>) that is specified in the <code>config.json</code> file inside the S3 bucket.</p> <ol> <li>Update hello world application, by opening <code>templates\\index.html</code> and changing <code>And we're live, one more time!</code> to <code>And we're live, one more time! v1.2.4</code> in line #183</li> <li>Build the docker image, tag and push it to the ECR repository.</li> </ol> <pre><code>docker build -t hello-world-apprunner-repo .\ndocker tag hello-world-apprunner-repo:latest &lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo:1.2.4\ndocker push &lt;&lt;account&gt;&gt;.dkr.ecr.us-east-1.amazonaws.com/hello-world-apprunner-repo:1.2.4\n</code></pre> <p>The above action will trigger an ECR event, which will get picked up by the Lambda function. The Lambda function will update the App Runner service with the new image in a few seconds. You can verify this by running the following command:</p> <pre><code>aws apprunner describe-service --service-arn arn:aws:apprunner:us-east-1:&lt;&lt;accountId&gt;:service/hello-world-service/&lt;&lt;serviceId&gt;&gt; | jq -r '.Service.Status'\n</code></pre> <p>Output</p> <pre><code>OPERATION_IN_PROGRESS\n</code></pre> <p>Event logs for the App Runner service will show the following, to confirm the update is being triggered by the deployed solution:</p> <pre><code>12-30-2022 01:55:48 PM [CI/CD] Semantic version &gt;1.2.2 matched with the recent ECR push 1.2.4, so updating the service to the deploy from the latest version\n</code></pre> <p>When the update is successful, you will see the following output with the changes applied to the App Runner service:</p> <p></p>"},{"location":"semantic-versioning-app-runner/#clean-up","title":"Clean Up","text":"<p>Run the following command from the root directory to delete the stack:</p> <pre><code>cdk destroy\naws ecr delete-repository --repository-name hello-world-apprunner-repo --force\naws iam delete-role --role-name ${ROLE_NAME}\n</code></pre>"},{"location":"semantic-versioning-app-runner/#considerations","title":"Considerations","text":"<p>Here are some essential items to consider before using this solution:</p> <ul> <li>The solution uses AWS App Runner APIs to update and deploy the new version of the application, so it is not a fully managed solution. The customer needs to manage the AWS CDK stack and the Lambda function.</li> <li>The solution does not support tracking <code>latest</code> tag. If the customer wants to track the latest or fixed tag, we recommend using the native CI/CD support in App Runner.</li> <li>The solution uses various AWS services (like Eventbridge, SQS, Lambda) to track the semantic version pattern. As the solution relays on Eventbridge events, SQS messages and Lambda invocations to track the semantic version, it can get expensive if the customer tracks multiple App Runner services and ECR repositories as it would result in multiple events, SQS messages and invocations. It can get expensive if the customer tracks multiple App Runner services and ECR repositories.</li> <li>The code is not production ready and is provided as is. The customer should test the solution in a non-production environment before using it.</li> <li>The solution does not support tracking multiple App Runner services using the same repository. If the customer wants to use the same repository for multiple App Runner services based on the semantic version, then the solution code needs to get updated to support this use case.</li> </ul>"},{"location":"semantic-versioning-app-runner/#conclusion","title":"Conclusion","text":"<p>This post showed how customers could power their release pipelines based on semantic versioning and deliver new versions of the application to their customers fully automatedly using App Runner.</p> <p>If you have any questions or feedback, please leave a comment below.</p>"},{"location":"semantic-versioning-app-runner/#references","title":"References","text":"<ul> <li>AWS App Runner</li> <li>Semantic Versioning</li> <li>AWS CDK</li> <li>Build a Continuous Delivery Pipeline for Your Container Images with Amazon ECR as Source</li> </ul>"},{"location":"semantic-versioning-app-runner/#hari-ohm-prasath","title":"Hari Ohm Prasath","text":"<p>Hari is a Senior Software Development Engineer with App Runner and Elastic Beanstalk, working on new ways for customers to modernize their web applications on AWS. Hari loves to code and actively contributes to the open source initiatives. You can find him in Medium, Github &amp; Twitter @hariohmprasath</p> <p></p>"},{"location":"semantic-versioning-app-runner/#elamaran-shanmugam","title":"Elamaran Shanmugam","text":"<p>Elamaran (Ela) Shanmugam is a Sr. Container Specialist Solutions Architect with Amazon Web Services. Ela is a Container, Observability and Multi-Account Architecture SME and helps AWS partners and customers to design and build scalable, secure and optimized container workloads on AWS. His passion is building and automating Infrastructure to allow customers to focus more on their business. He is based out of Tampa, Florida and you can reach him on twitter @IamElaShan</p> <p></p>"},{"location":"semantic-versioning-app-runner/#re-alvarez-parmar","title":"Re Alvarez Parmar","text":"<p>In his role as Containers Specialist Solutions Architect at Amazon Web Services. Re advises engineering teams with modernizing and building distributed services in the cloud. Prior to joining AWS, he spent over 15 years as Enterprise and Software Architect. He is based out of Seattle. You can connect with him on LinkedIn linkedin.com/in/realvarez/</p>"}]}